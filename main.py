#!/usr/bin/env python3
"""
Chattea Intent Classifier
Run with: python main.py

Required files:
- chattea_dataset.csv
- responses.json

First run: Trains model and saves to cnn_chattea.pth
Subsequent runs: Loads pre-trained model for instant inference
"""

import json
import pandas as pd
import re
import os
import torch
import torch.nn as nn
from sentence_transformers import SentenceTransformer, util
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from difflib import get_close_matches
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    # File paths
    DATASET_PATH = "chatbot_dataset.csv"
    RESPONSES_PATH = "responses.json"
    MODEL_PATH = "chattea.pth"
    
    # Training parameters
    BATCH_SIZE = 32
    EPOCHS = 50
    LEARNING_RATE = 0.001
    TEST_SIZE = 0.2
    RANDOM_SEED = 42
    
    # Model parameters
    EMBED_DIM = 384
    HIDDEN_DIM_1 = 256
    HIDDEN_DIM_2 = 128
    DROPOUT = 0.3
    
    # Inference parameters
    CONFIDENCE_THRESHOLD = 0.90

config = Config()

# ============================================================================
# DEVICE SETUP
# ============================================================================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("=" * 80)
print("CHATTEA INTENT CLASSIFIER")
print("=" * 80)
print(f"Device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
print("=" * 80)

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================

class EmbeddingClassifier(nn.Module):
    """
    Feedforward Neural Network for Intent Classification
    
    Why Feedforward (not CNN)?
    - Sentence embeddings are feature vectors, not sequences
    - MLP treats each dimension as independent feature
    - CNN would incorrectly assume spatial relationships
    """
    
    def __init__(self, embed_dim=384, num_classes=33):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        return self.network(x)


# ============================================================================
# FUZZY MATCHING
# ============================================================================

def build_vocabulary(texts):
    """Extract all unique words from texts"""
    vocab = set()
    for text in texts:
        vocab.update(re.findall(r'\w+', text.lower()))
    return vocab

def fuzzy_correct(text, vocab, cutoff=0.8):
    """Correct typos using Levenshtein distance"""
    words = re.findall(r'\w+', text.lower())
    corrected = []
    
    for word in words:
        matches = get_close_matches(word, vocab, n=1, cutoff=cutoff)
        corrected.append(matches[0] if matches else word)
    
    result = text
    for orig, corr in zip(words, corrected):
        if orig != corr:
            result = re.sub(rf'\b{orig}\b', corr, result, count=1, flags=re.IGNORECASE)
    
    return result

# ============================================================================
# TRAINING
# ============================================================================

def create_batches(X, y, batch_size):
    """Create mini-batches with shuffling"""
    indices = torch.randperm(len(X))
    for i in range(0, len(X), batch_size):
        batch_idx = indices[i:i+batch_size]
        yield X[batch_idx], y[batch_idx]

def train_model(X_train, y_train, X_val, y_val, num_classes):
    """Train the classifier"""
    print("\n" + "=" * 80)
    print("TRAINING MODEL")
    print("=" * 80)
    
    print("Using Feedforward architecture")
    model = EmbeddingClassifier(
        embed_dim=config.EMBED_DIM,
        num_classes=num_classes
    ).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()
    
    best_val_acc = 0
    
    print("\nEpoch | Train Acc | Train Loss | Val Acc | Val Loss")
    print("-" * 65)
    
    for epoch in range(config.EPOCHS):
        # Training
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for batch_X, batch_y in create_batches(X_train, y_train, config.BATCH_SIZE):
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_correct += (outputs.argmax(1) == batch_y).sum().item()
            train_total += len(batch_y)
        
        train_acc = train_correct / train_total
        train_loss = train_loss / (len(X_train) // config.BATCH_SIZE + 1)
        
        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val).item()
            val_acc = (val_outputs.argmax(1) == y_val).float().mean().item()
        
        # Print progress
        if epoch % 5 == 0 or epoch == config.EPOCHS - 1:
            print(f"{epoch:5d} | {train_acc:9.4f} | {train_loss:10.4f} | {val_acc:7.4f} | {val_loss:8.4f}")
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), config.MODEL_PATH)
    
    print("\n" + "=" * 80)
    print(f"âœ“ Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")
    print(f"âœ“ Model saved to: {config.MODEL_PATH}")
    print("=" * 80)
    
    return model

# ============================================================================
# CHATBOT CLASS
# ============================================================================

class ChatteaBot:
    """Main chatbot class"""
    
    def __init__(self, model, embedder, label_encoder, responses, df, sentence_embeddings, vocab):
        self.model = model
        self.embedder = embedder
        self.label_encoder = label_encoder
        self.responses = responses
        self.df = df
        self.sentence_embeddings = sentence_embeddings
        self.vocab = vocab
        
        self.model.eval()
        self.intent_map = dict(enumerate(label_encoder.classes_))
    
    def get_reply(self, user_input, debug=False):
        """Get chatbot response with optional debug output"""
        text = user_input.strip().lower()
        
        # Rule-based filters for greetings only
        if any(g in text for g in ["hai", "halo", "hello", "hi", "hey", "pagi", "siang", "malam"]):
            if debug:
                self._print_debug(user_input, "greeting", 1.0, "greeting", 1.0, "greeting", "RULE-BASED")
            return self._get_response("greeting")
        
        # Classification
        with torch.no_grad():
            user_emb = self.embedder.encode(user_input, convert_to_tensor=True).to(device)
            user_emb = user_emb.unsqueeze(0)
            
            # Model prediction
            logits = self.model(user_emb)
            probs = logits.softmax(1)
            confidence = probs.max().item()
            intent_idx = logits.argmax(1).item()
            model_intent = self.intent_map[intent_idx]
            
            # Retrieval fallback
            cos_scores = util.cos_sim(user_emb, self.sentence_embeddings)[0]
            best_match_idx = cos_scores.argmax().item()
            retrieval_intent = self.df.iloc[best_match_idx]['intent']
            retrieval_score = cos_scores[best_match_idx].item()
            
            # Choose final intent
            if confidence > config.CONFIDENCE_THRESHOLD:
                final_intent = model_intent
                decision = "MODEL"
            else:
                final_intent = retrieval_intent
                decision = "RETRIEVAL"
        
        # Debug output
        if debug:
            self._print_debug(user_input, model_intent, confidence, retrieval_intent, 
                            retrieval_score, final_intent, decision)
        
        return self._get_response(final_intent)
    
    def _print_debug(self, query, model_intent, model_conf, retrieval_intent, 
                     retrieval_score, final_intent, decision):
        """Print detailed debug information"""
        print("\n" + "=" * 80)
        print(f"QUERY         : {query}")
        print(f"Model Predict : {model_intent:<20} Confidence: {model_conf:.4f} ({model_conf*100:6.2f}%)")
        print(f"Threshold     : {config.CONFIDENCE_THRESHOLD} â†’ Use Model?: {'YES' if model_conf > config.CONFIDENCE_THRESHOLD else 'NO'}")
        print(f"Retrieval     : {retrieval_intent:<20} Score: {retrieval_score:.4f}")
        print(f"FINAL INTENT  : â†’ {final_intent} â† (Source: {decision})")
        print("=" * 80)
    
    def _get_response(self, intent):
        """Get response for intent"""
        response = self.responses.get(intent, self.responses.get("help", "I'm not sure how to help with that."))
        
        if isinstance(response, dict):
            return response.get("en", response.get("id", "I'm not sure how to help with that."))
        
        return response

# ============================================================================
# MAIN PIPELINE
# ============================================================================

def main():
    """Main execution pipeline"""
    
    # Load data
    print("\nðŸ“‚ Loading data...")
    df = pd.read_csv(config.DATASET_PATH)
    
    with open(config.RESPONSES_PATH, "r", encoding="utf-8") as f:
        responses = json.load(f)
    
    print(f"âœ“ Loaded {len(df)} samples, {df['intent'].nunique()} intents")
    
    # Build vocabulary
    print("\nðŸ“š Building vocabulary...")
    vocab = build_vocabulary(df['text'])
    print(f"âœ“ Vocabulary: {len(vocab)} words")
    
    # Label encoding
    print("\nðŸ·ï¸  Encoding labels...")
    le = LabelEncoder()
    df['label'] = le.fit_transform(df['intent'])
    num_classes = len(le.classes_)
    print(f"âœ“ Classes: {num_classes}")
    
    # Load embedder
    print("\nðŸ§  Loading sentence transformer...")
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    print("âœ“ Embedder loaded")
    
    # Generate embeddings
    print("\nðŸ“Š Generating embeddings...")
    sentence_embeddings = embedder.encode(
        df['text'].tolist(),
        convert_to_tensor=True,
        show_progress_bar=True
    ).to(device)
    print(f"âœ“ Embeddings shape: {sentence_embeddings.shape}")
    
    # Train or load model
    if os.path.exists(config.MODEL_PATH):
        print(f"\nâœ“ Found existing model: {config.MODEL_PATH}")
        print("Loading pre-trained model...")
        model = EmbeddingClassifier(embed_dim=config.EMBED_DIM, num_classes=num_classes).to(device)
        model.load_state_dict(torch.load(config.MODEL_PATH, map_location=device))
        model.eval()
        print("âœ“ Model loaded!")
    else:
        print("\nâš ï¸  No pre-trained model found. Training from scratch...")
        
        # Prepare data
        X = sentence_embeddings.to(device)
        y = torch.tensor(df['label'].values, dtype=torch.long).to(device)
        
        # Split data
        train_idx, val_idx = train_test_split(
            list(range(len(X))),
            test_size=config.TEST_SIZE,
            random_state=config.RANDOM_SEED,
            stratify=y.cpu()
        )
        
        X_train = X[train_idx]
        X_val = X[val_idx]
        y_train = y[train_idx]
        y_val = y[val_idx]
        
        # Train
        model = train_model(X_train, y_train, X_val, y_val, num_classes)
        model.load_state_dict(torch.load(config.MODEL_PATH))
    
    # Create bot
    print("\nðŸ¤– Initializing chatbot...")
    bot = ChatteaBot(model, embedder, le, responses, df, sentence_embeddings, vocab)
    print("âœ“ Chatbot ready!")
    
    # Test queries
    print("\n" + "=" * 80)
    print("ðŸ§ª TESTING")
    print("=" * 80)
    
    test_queries = [
        "hello",
        "what is chattea",
        "how to blast message",
        "check 08123456789",
        "create instance"
    ]
    
    for query in test_queries:
        print(f"\nðŸ‘¤ User: {query}")
        response = bot.get_reply(query, debug=False)
        print(f"ðŸ¤– Bot: {response[:100]}{'...' if len(response) > 100 else ''}")

    
    # Interactive mode
    print("\n" + "=" * 80)
    print("ðŸ’¬ INTERACTIVE MODE")
    print("=" * 80)
    print("Type your messages (or 'quit'/'exit'/'q' to exit)")
    
    while True:
        try:
            user_input = input("You: ").strip()
            
            # Exit commands only - don't use chatbot for these
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("\nðŸ‘‹ Exiting Chattea. Goodbye!")
                break
            
            if not user_input:
                continue
            
            response = bot.get_reply(user_input, debug=True)
            print(f"\nBot: {response}\n")
            
        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Exiting Chattea. Goodbye!")
            break
        except Exception as e:
            print(f"Error: {e}")
            continue

if __name__ == "__main__":
    main()