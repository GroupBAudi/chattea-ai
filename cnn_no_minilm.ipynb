{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Chattea Intent Classifier - CNN + Word2Vec (Optimized)\n",
    "\n",
    "Run:\n",
    "    python chattea_cnn.py\n",
    "\n",
    "Required files (same directory):\n",
    "- chatbot_dataset.csv  (must contain 'text' and 'intent' columns)\n",
    "- responses.json       (bilingual or single-language responses)\n",
    "\n",
    "Outputs saved:\n",
    "- word2vec.model\n",
    "- chattea.pth\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from difflib import get_close_matches, SequenceMatcher\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # File paths\n",
    "    DATASET_PATH = \"chatbot_dataset.csv\"\n",
    "    RESPONSES_PATH = \"responses.json\"\n",
    "    MODEL_PATH = \"cnn_chattea.pth\"\n",
    "    WORD2VEC_PATH = \"word2vec.model\"\n",
    "\n",
    "    # Word2Vec parameters (OPTIMIZED)\n",
    "    EMBEDDING_DIM = 100        # Embedding dimension\n",
    "    WORD2VEC_WINDOW = 5        # Context window\n",
    "    WORD2VEC_MIN_COUNT = 1     # Minimum word frequency\n",
    "    WORD2VEC_SG = 1            # Skip-gram (better for small datasets)\n",
    "\n",
    "    # CNN parameters (OPTIMIZED)\n",
    "    NUM_FILTERS = 128          # Filters per kernel\n",
    "    KERNEL_SIZES = [2, 3, 4]   # Includes 2-word phrases!\n",
    "    DROPOUT = 0.5              # Higher regularization\n",
    "    MAX_SEQ_LENGTH = 20        # Shorter = more efficient\n",
    "\n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 30\n",
    "    LEARNING_RATE = 0.001\n",
    "    TEST_SIZE = 0.2\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # Inference parameters\n",
    "    FUZZY_CUTOFF = 0.8\n",
    "    CONFIDENCE_THRESHOLD = 0.75\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(Config.RANDOM_SEED)\n",
    "np.random.seed(Config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2629271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET SANITY CHECK & EXPLORATORY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä DATASET SANITY CHECK & ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(Config.DATASET_PATH)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  BASIC STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total intents: {df['intent'].nunique()}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n2Ô∏è‚É£  DATA QUALITY\")\n",
    "print(\"-\" * 80)\n",
    "missing = df.isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col}: {missing[col]} ({missing[col]/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates} ({duplicates/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Intent distribution\n",
    "print(\"\\n3Ô∏è‚É£  INTENT DISTRIBUTION\")\n",
    "print(\"-\" * 80)\n",
    "intent_counts = df['intent'].value_counts()\n",
    "print(intent_counts)\n",
    "print(f\"\\nMost common: {intent_counts.index[0]} ({intent_counts.iloc[0]} samples)\")\n",
    "print(f\"Least common: {intent_counts.index[-1]} ({intent_counts.iloc[-1]} samples)\")\n",
    "print(f\"Class imbalance ratio: {intent_counts.iloc[0] / intent_counts.iloc[-1]:.2f}x\")\n",
    "\n",
    "# Text length analysis\n",
    "print(\"\\n4Ô∏è‚É£  TEXT LENGTH ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(f\"Character length:\")\n",
    "print(f\"  Min: {df['text_length'].min()}\")\n",
    "print(f\"  Max: {df['text_length'].max()}\")\n",
    "print(f\"  Mean: {df['text_length'].mean():.2f}\")\n",
    "print(f\"  Median: {df['text_length'].median():.0f}\")\n",
    "\n",
    "print(f\"\\nWord count:\")\n",
    "print(f\"  Min: {df['word_count'].min()}\")\n",
    "print(f\"  Max: {df['word_count'].max()}\")\n",
    "print(f\"  Mean: {df['word_count'].mean():.2f}\")\n",
    "print(f\"  Median: {df['word_count'].median():.0f}\")\n",
    "\n",
    "# Find longest sentences\n",
    "print(\"\\n5Ô∏è‚É£  LONGEST SENTENCES (Top 5)\")\n",
    "print(\"-\" * 80)\n",
    "longest = df.nlargest(5, 'word_count')[['text', 'intent', 'word_count']]\n",
    "for idx, row in longest.iterrows():\n",
    "    print(f\"\\n[{row['word_count']} words] Intent: {row['intent']}\")\n",
    "    print(f\"Text: {row['text']}\")\n",
    "\n",
    "# Find shortest sentences\n",
    "print(\"\\n6Ô∏è‚É£  SHORTEST SENTENCES (Top 5)\")\n",
    "print(\"-\" * 80)\n",
    "shortest = df.nsmallest(5, 'word_count')[['text', 'intent', 'word_count']]\n",
    "for idx, row in shortest.iterrows():\n",
    "    print(f\"\\n[{row['word_count']} words] Intent: {row['intent']}\")\n",
    "    print(f\"Text: {row['text']}\")\n",
    "\n",
    "# Vocabulary analysis\n",
    "print(\"\\n7Ô∏è‚É£  VOCABULARY STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "all_words = []\n",
    "for text in df['text']:\n",
    "    all_words.extend(str(text).lower().split())\n",
    "\n",
    "unique_words = set(all_words)\n",
    "print(f\"Total words (with repetition): {len(all_words)}\")\n",
    "print(f\"Unique words: {len(unique_words)}\")\n",
    "print(f\"Vocabulary richness: {len(unique_words)/len(all_words):.4f}\")\n",
    "\n",
    "# Most common words\n",
    "from collections import Counter\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"\\nMost common words (Top 10):\")\n",
    "for word, count in word_freq.most_common(10):\n",
    "    print(f\"  '{word}': {count} times\")\n",
    "\n",
    "# Justification for hyperparameters\n",
    "print(\"\\n8Ô∏è‚É£  HYPERPARAMETER JUSTIFICATION\")\n",
    "print(\"-\" * 80)\n",
    "max_words = df['word_count'].max()\n",
    "mean_words = df['word_count'].mean()\n",
    "percentile_95 = df['word_count'].quantile(0.95)\n",
    "\n",
    "print(f\"‚úì MAX_SEQ_LENGTH = {Config.MAX_SEQ_LENGTH}\")\n",
    "print(f\"  Rationale: 95th percentile = {percentile_95:.0f} words\")\n",
    "print(f\"  Only {(df['word_count'] > Config.MAX_SEQ_LENGTH).sum()} samples ({(df['word_count'] > Config.MAX_SEQ_LENGTH).sum()/len(df)*100:.2f}%) exceed this length\")\n",
    "\n",
    "print(f\"\\n‚úì EMBEDDING_DIM = {Config.EMBEDDING_DIM}\")\n",
    "print(f\"  Rationale: Vocabulary size = {len(unique_words)}\")\n",
    "print(f\"  Rule of thumb: embedding_dim ‚âà vocab_size^0.25 = {len(unique_words)**0.25:.0f}\")\n",
    "print(f\"  100 dimensions provides good balance for vocab of ~1000 words\")\n",
    "\n",
    "print(f\"\\n‚úì KERNEL_SIZES = {Config.KERNEL_SIZES}\")\n",
    "print(f\"  Rationale: Mean sentence length = {mean_words:.1f} words\")\n",
    "print(f\"  Kernels [2,3,4] capture 2-4 word phrases (n-grams)\")\n",
    "print(f\"  Examples: 'send message' (2), 'how to send' (3), 'create new instance now' (4)\")\n",
    "\n",
    "print(f\"\\n‚úì BATCH_SIZE = {Config.BATCH_SIZE}\")\n",
    "print(f\"  Rationale: Dataset size = {len(df)} samples\")\n",
    "print(f\"  {len(df)//Config.BATCH_SIZE} batches per epoch\")\n",
    "print(f\"  Provides good gradient estimation without excessive memory usage\")\n",
    "\n",
    "print(f\"\\n‚úì DROPOUT = {Config.DROPOUT}\")\n",
    "print(f\"  Rationale: Small dataset ({len(df)} samples) ‚Üí high overfitting risk\")\n",
    "print(f\"  Higher dropout (0.5) provides aggressive regularization\")\n",
    "\n",
    "# Class balance visualization\n",
    "print(\"\\n9Ô∏è‚É£  CLASS BALANCE CHECK\")\n",
    "print(\"-\" * 80)\n",
    "min_samples = intent_counts.min()\n",
    "max_samples = intent_counts.max()\n",
    "imbalance = max_samples / min_samples\n",
    "\n",
    "if imbalance < 1.5:\n",
    "    print(\"‚úì Classes are WELL BALANCED (ratio < 1.5x)\")\n",
    "elif imbalance < 3:\n",
    "    print(\"‚ö†Ô∏è  Classes are MODERATELY IMBALANCED (ratio 1.5-3x)\")\n",
    "else:\n",
    "    print(\"‚ùå Classes are SEVERELY IMBALANCED (ratio > 3x)\")\n",
    "    print(\"   Consider: class weighting, oversampling minority, or undersampling majority\")\n",
    "\n",
    "print(f\"   Imbalance ratio: {imbalance:.2f}x\")\n",
    "\n",
    "# Sample queries per intent\n",
    "print(\"\\nüîü SAMPLE QUERIES PER INTENT (3 examples each)\")\n",
    "print(\"-\" * 80)\n",
    "for intent in df['intent'].unique()[:5]:  # Show first 5 intents\n",
    "    print(f\"\\nüìå Intent: {intent}\")\n",
    "    samples = df[df['intent'] == intent]['text'].head(3).tolist()\n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        print(f\"   {i}. {sample}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DATASET SANITY CHECK COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean up temporary columns\n",
    "df = df.drop(['text_length', 'word_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf454f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHATTEA INTENT CLASSIFIER - CNN + WORD2VEC\n",
      "================================================================================\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DEVICE SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHATTEA INTENT CLASSIFIER - CNN + WORD2VEC\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEXT PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text into words\"\"\"\n",
    "    return clean_text(text).split()\n",
    "\n",
    "def build_vocabulary(texts):\n",
    "    \"\"\"Extract all unique words from texts\"\"\"\n",
    "    vocab = set()\n",
    "    for text in texts:\n",
    "        vocab.update(re.findall(r'\\w+', str(text).lower()))\n",
    "    return vocab\n",
    "\n",
    "def fuzzy_correct(text, vocab, cutoff=Config.FUZZY_CUTOFF):\n",
    "    \"\"\"Correct typos using difflib.get_close_matches\"\"\"\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    corrected = []\n",
    "    for word in words:\n",
    "        matches = get_close_matches(word, vocab, n=1, cutoff=cutoff)\n",
    "        corrected.append(matches[0] if matches else word)\n",
    "    return ' '.join(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WORD2VEC EMBEDDER\n",
    "# ============================================================================\n",
    "\n",
    "class Word2VecEmbedder:\n",
    "    \"\"\"Word2Vec embedding wrapper with proper initialization\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {}\n",
    "        self.embedding_matrix = None\n",
    "        self.vocab_size = 0\n",
    "        self.embed_dim = Config.EMBEDDING_DIM\n",
    "\n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train Word2Vec on tokenized sentences\"\"\"\n",
    "        print(\"\\nüß† Training Word2Vec...\")\n",
    "        self.model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            vector_size=Config.EMBEDDING_DIM,\n",
    "            window=Config.WORD2VEC_WINDOW,\n",
    "            min_count=Config.WORD2VEC_MIN_COUNT,\n",
    "            sg=Config.WORD2VEC_SG,\n",
    "            seed=Config.RANDOM_SEED,\n",
    "            workers=4\n",
    "        )\n",
    "\n",
    "        idx = 2\n",
    "        for word in self.model.wv.index_to_key:\n",
    "            self.word2idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "        self.embedding_matrix = np.zeros((self.vocab_size, self.embed_dim), dtype=np.float32)\n",
    "\n",
    "        for word, idx in self.word2idx.items():\n",
    "            if word in ['<PAD>', '<UNK>']:\n",
    "                if word == '<UNK>':\n",
    "                    self.embedding_matrix[idx] = np.random.randn(self.embed_dim) * 0.01\n",
    "            else:\n",
    "                try:\n",
    "                    self.embedding_matrix[idx] = self.model.wv[word]\n",
    "                except KeyError:\n",
    "                    self.embedding_matrix[idx] = np.random.randn(self.embed_dim) * 0.01\n",
    "\n",
    "        print(f\"‚úì Word2Vec trained: vocab={self.vocab_size}, dim={self.embed_dim}\")\n",
    "        return self\n",
    "\n",
    "    def save(self, path=Config.WORD2VEC_PATH):\n",
    "        if self.model:\n",
    "            self.model.save(path)\n",
    "            print(f\"‚úì Word2Vec saved to {path}\")\n",
    "\n",
    "    def load(self, path=Config.WORD2VEC_PATH):\n",
    "        print(f\"\\nüß† Loading Word2Vec from {path}...\")\n",
    "        self.model = Word2Vec.load(path)\n",
    "\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        idx = 2\n",
    "        for word in self.model.wv.index_to_key:\n",
    "            self.word2idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "        self.embedding_matrix = np.zeros((self.vocab_size, self.embed_dim), dtype=np.float32)\n",
    "\n",
    "        for word, idx in self.word2idx.items():\n",
    "            if word in ['<PAD>', '<UNK>']:\n",
    "                if word == '<UNK>':\n",
    "                    self.embedding_matrix[idx] = np.random.randn(self.embed_dim) * 0.01\n",
    "            else:\n",
    "                try:\n",
    "                    self.embedding_matrix[idx] = self.model.wv[word]\n",
    "                except KeyError:\n",
    "                    self.embedding_matrix[idx] = np.random.randn(self.embed_dim) * 0.01\n",
    "\n",
    "        print(f\"‚úì Word2Vec loaded: vocab={self.vocab_size}, dim={self.embed_dim}\")\n",
    "        return self\n",
    "\n",
    "    def encode_sequence(self, tokens, max_length=Config.MAX_SEQ_LENGTH):\n",
    "        indices = [self.word2idx.get(token, self.word2idx[\"<UNK>\"]) for token in tokens[:max_length]]\n",
    "        while len(indices) < max_length:\n",
    "            indices.append(self.word2idx[\"<PAD>\"])\n",
    "        return indices\n",
    "\n",
    "    def sentence_vector(self, tokens):\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2idx and token not in (\"<PAD>\", \"<UNK>\"):\n",
    "                idx = self.word2idx[token]\n",
    "                if idx < len(self.embedding_matrix):\n",
    "                    vectors.append(self.embedding_matrix[idx])\n",
    "        if len(vectors) == 0:\n",
    "            return np.zeros(self.embed_dim, dtype=np.float32)\n",
    "        return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CNN MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    \"\"\"CNN for Text Classification (Kim, 2014)\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "            print(\"‚úì CNN initialized with Word2Vec embeddings!\")\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=Config.NUM_FILTERS, kernel_size=k)\n",
    "            for k in Config.KERNEL_SIZES\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(Config.DROPOUT)\n",
    "        self.fc = nn.Linear(Config.NUM_FILTERS * len(Config.KERNEL_SIZES), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)                       # (batch, seq_len, embed_dim)\n",
    "        embedded = embedded.transpose(1, 2)                # (batch, embed_dim, seq_len)\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = F.relu(conv(embedded))              # (batch, num_filters, L)\n",
    "            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)\n",
    "            conv_outputs.append(pooled)\n",
    "        concatenated = torch.cat(conv_outputs, dim=1)\n",
    "        dropped = self.dropout(concatenated)\n",
    "        logits = self.fc(dropped)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ae515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class IntentDataset(Dataset):\n",
    "    \"\"\"Simple dataset wrapper\"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4169359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CNN\n",
    "# ============================================================================\n",
    "\n",
    "def train_model_pretty(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train the CNN classifier with MLP-style formatted output\"\"\"\n",
    "    model = model.to(config.DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataset = IntentDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üèãÔ∏è  TRAINING LOOP (WITH PROPER BATCHING!)\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"Epoch | Train Acc | Train Loss | Val Acc | Val Loss\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(config.DEVICE)\n",
    "            batch_y = batch_y.to(config.DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == batch_y).sum().item()\n",
    "            train_total += len(batch_y)\n",
    "\n",
    "        train_acc = train_correct / (train_total + 1e-12)\n",
    "        avg_train_loss = train_loss / (len(train_loader) + 1e-12)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_device = X_val.to(config.DEVICE)\n",
    "            y_val_device = y_val.to(config.DEVICE)\n",
    "            val_outputs = model(X_val_device)\n",
    "            val_loss = criterion(val_outputs, y_val_device).item()\n",
    "            val_acc = (val_outputs.argmax(1) == y_val_device).float().mean().item()\n",
    "\n",
    "        # Print progress (every 5 epochs + last)\n",
    "        if epoch % 5 == 0 or epoch == Config.EPOCHS - 1:\n",
    "            print(f\"{epoch:5d} | {train_acc:9.4f} | {avg_train_loss:10.4f} | {val_acc:7.4f} | {val_loss:8.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), Config.MODEL_PATH)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"‚úì Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "    print(f\"‚úì Model saved to: {Config.MODEL_PATH}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(Config.MODEL_PATH, map_location=config.DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    # Final evaluation on train and val\n",
    "    with torch.no_grad():\n",
    "        final_train_pred = model(X_train.to(config.DEVICE)).argmax(1).cpu()\n",
    "        final_train_acc = (final_train_pred == y_train).float().mean().item()\n",
    "\n",
    "        final_val_pred = model(X_val.to(config.DEVICE)).argmax(1).cpu()\n",
    "        final_val_acc = (final_val_pred == y_val).float().mean().item()\n",
    "\n",
    "    print(\"\\n   Training Accuracy:   {:.4f} ({:.2f}%)\".format(final_train_acc, final_train_acc*100))\n",
    "    print(\"   Validation Accuracy: {:.4f} ({:.2f}%)\".format(final_val_acc, final_val_acc*100))\n",
    "    print(\"\\n‚úì Model ready for inference!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEBUG PRINTING\n",
    "# ============================================================================\n",
    "\n",
    "def print_debug(query, model_intent, model_conf, retrieval_intent,\n",
    "                retrieval_score, final_intent, decision):\n",
    "    \"\"\"Print detailed debug information\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"QUERY         : {query}\")\n",
    "    print(f\"Model Predict : {model_intent:<20} Confidence: {model_conf:.4f} ({model_conf*100:6.2f}%)\")\n",
    "    print(f\"Threshold     : {Config.CONFIDENCE_THRESHOLD} ‚Üí Use Model?: {'YES' if model_conf > Config.CONFIDENCE_THRESHOLD else 'NO'}\")\n",
    "    print(f\"Retrieval     : {retrieval_intent:<20} Score: {retrieval_score:.4f}\")\n",
    "    print(f\"FINAL INTENT  : ‚Üí {final_intent} ‚Üê (Source: {decision})\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddded902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHATBOT CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ChatteaBot:\n",
    "    \"\"\"Main chatbot class with hybrid classification\"\"\"\n",
    "\n",
    "    def __init__(self, model, embedder, label_encoder, responses,\n",
    "                 df, sentence_vectors, vocab):\n",
    "        self.model = model\n",
    "        self.embedder = embedder\n",
    "        self.le = label_encoder  # sklearn LabelEncoder instance\n",
    "        self.responses = responses\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.sentence_vectors = sentence_vectors.astype(np.float32) if sentence_vectors is not None else np.zeros((len(self.df), embedder.embed_dim))\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Intent mapping\n",
    "        if hasattr(self.le, \"classes_\"):\n",
    "            self.intent_map = {i: label for i, label in enumerate(self.le.classes_)}\n",
    "        elif isinstance(self.le, dict):\n",
    "            self.intent_map = {v: k for k, v in self.le.items()}\n",
    "        else:\n",
    "            self.intent_map = {}\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def _get_response(self, intent):\n",
    "        \"\"\"Get response for intent\"\"\"\n",
    "        response = self.responses.get(intent, self.responses.get(\"help\", \"I'm not sure how to help with that.\"))\n",
    "        if isinstance(response, dict):\n",
    "            return response.get(\"en\", response.get(\"id\", next(iter(response.values()))))\n",
    "        return response\n",
    "\n",
    "    def get_reply(self, user_input, debug=False):\n",
    "        \"\"\"Get chatbot response with optional debug output\"\"\"\n",
    "        text = str(user_input).strip()\n",
    "\n",
    "        if text == \"\":\n",
    "            return \"Say something :)\"\n",
    "\n",
    "        # Rule-based greeting\n",
    "        if any(g in text.lower() for g in [\"hai\", \"halo\", \"hello\", \"hi\", \"hey\", \"pagi\", \"siang\", \"malam\"]):\n",
    "            if debug:\n",
    "                print_debug(user_input, \"greeting\", 1.0, \"greeting\", 1.0, \"greeting\", \"RULE-BASED\")\n",
    "            return self._get_response(\"greeting\")\n",
    "\n",
    "        # Fuzzy correction\n",
    "        corrected = fuzzy_correct(text, self.vocab, Config.FUZZY_CUTOFF)\n",
    "        tokens = tokenize(corrected)\n",
    "\n",
    "        # Model prediction\n",
    "        sequence = self.embedder.encode_sequence(tokens, Config.MAX_SEQ_LENGTH)\n",
    "        x = torch.LongTensor([sequence]).to(config.DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(x)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "            model_conf = float(probs.max())\n",
    "            model_idx = int(np.argmax(probs))\n",
    "            # map to label string\n",
    "            try:\n",
    "                model_intent = self.intent_map[model_idx]\n",
    "            except Exception:\n",
    "                model_intent = str(model_idx)\n",
    "\n",
    "        # Retrieval fallback (sentence vectors from embedder average)\n",
    "        user_vec = self.embedder.sentence_vector(tokens).reshape(1, -1)\n",
    "\n",
    "        if self.sentence_vectors is None or len(self.sentence_vectors) == 0:\n",
    "            retrieval_intent = model_intent\n",
    "            retrieval_score = 0.0\n",
    "        else:\n",
    "            similarities = cosine_similarity(user_vec, self.sentence_vectors)[0]\n",
    "            best_idx = int(np.argmax(similarities))\n",
    "            retrieval_score = float(similarities[best_idx])\n",
    "            retrieval_intent = str(self.df.iloc[best_idx][\"intent\"])\n",
    "\n",
    "        # Decision\n",
    "        if model_conf >= Config.CONFIDENCE_THRESHOLD:\n",
    "            final_intent = model_intent\n",
    "            decision = \"MODEL\"\n",
    "        else:\n",
    "            final_intent = retrieval_intent\n",
    "            decision = \"RETRIEVAL\"\n",
    "\n",
    "        if debug:\n",
    "            print_debug(user_input, model_intent, model_conf, retrieval_intent,\n",
    "                        retrieval_score, final_intent, decision)\n",
    "\n",
    "        return self._get_response(final_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a456168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILS: Pretty evaluation & inference output\n",
    "# ============================================================================\n",
    "\n",
    "def pretty_inference_tests(bot, test_queries=None):\n",
    "    if test_queries is None:\n",
    "        test_queries = [\n",
    "            \"hello\",\n",
    "            \"what is chattea\",\n",
    "            \"how to blast message\",\n",
    "            \"create instance\",\n",
    "            \"send bulk messages\"\n",
    "        ]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üß™ TESTING INFERENCE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nRunning test queries:\\n\")\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"üë§ User: {query}\")\n",
    "        try:\n",
    "            response = bot.get_reply(query, debug=False)\n",
    "            # truncate like your example\n",
    "            out = response if isinstance(response, str) else str(response)\n",
    "            print(f\"ü§ñ Bot: {out[:200]}{'...' if len(out) > 200 else ''}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error during inference:\", e)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def pretty_evaluation(model, X_val, y_val, le, df):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val.to(config.DEVICE))\n",
    "        preds = outputs.argmax(1).cpu().numpy()\n",
    "        labels = y_val.numpy()\n",
    "\n",
    "    val_acc = accuracy_score(labels, preds)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä MODEL EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚úì Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\\n\")\n",
    "    print(\"üìã Per-Intent Performance:\")\n",
    "\n",
    "    intent_names = list(le.classes_)\n",
    "    for i, intent_name in enumerate(intent_names):\n",
    "        mask = labels == i\n",
    "        count = int(mask.sum())\n",
    "        if count == 0:\n",
    "            continue\n",
    "        intent_acc = (preds[mask] == labels[mask]).mean()\n",
    "        print(f\"   {intent_name:30s}: {intent_acc:.3f} ({count:2d} samples)\")\n",
    "\n",
    "    # Full training set accuracy if available as X_all global\n",
    "    try:\n",
    "        if 'X' in globals():\n",
    "            with torch.no_grad():\n",
    "                all_outputs = model(X.to(config.DEVICE))\n",
    "                all_preds = all_outputs.argmax(1).cpu().numpy()\n",
    "                all_labels = np.array([int(x) for x in df['label'].values])\n",
    "                train_acc = (all_preds == all_labels).mean()\n",
    "                print(f\"\\nAccuracy on FULL training set: {train_acc:.4f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa9c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Loading data...\n",
      "‚úì Loaded 2102 samples, 14 intents\n",
      "\n",
      "üìö Building vocabulary...\n",
      "‚úì Vocabulary: 1037 words\n",
      "\n",
      "üè∑Ô∏è  Encoding labels...\n",
      "‚úì Classes: 14\n",
      "\n",
      "‚úÇÔ∏è  Tokenizing...\n",
      "\n",
      "üß† Training Word2Vec...\n",
      "‚úì Word2Vec trained: vocab=1043, dim=100\n",
      "‚úì Word2Vec saved to word2vec.model\n",
      "\n",
      "üìä Preparing sequences...\n",
      "‚úì CNN initialized with Word2Vec embeddings!\n",
      "\n",
      "‚ö†Ô∏è  No pre-trained model found. Training from scratch...\n",
      "\n",
      "================================================================================\n",
      "üèãÔ∏è  TRAINING LOOP (WITH PROPER BATCHING!)\n",
      "================================================================================\n",
      "\n",
      "Epoch | Train Acc | Train Loss | Val Acc | Val Loss\n",
      "-----------------------------------------------------------------\n",
      "    0 |    0.0756 |     2.5957 |  0.1164 |   2.5480\n",
      "    5 |    0.9423 |     0.2226 |  0.9572 |   0.1690\n",
      "   10 |    0.9958 |     0.0300 |  0.9810 |   0.0742\n",
      "   15 |    0.9994 |     0.0072 |  0.9810 |   0.0602\n",
      "   20 |    1.0000 |     0.0052 |  0.9857 |   0.0614\n",
      "   25 |    0.9976 |     0.0066 |  0.9905 |   0.0568\n",
      "   29 |    1.0000 |     0.0035 |  0.9881 |   0.0564\n",
      "\n",
      "================================================================================\n",
      "‚úì Best Validation Accuracy: 0.9905 (99.05%)\n",
      "‚úì Model saved to: chattea.pth\n",
      "================================================================================\n",
      "\n",
      "   Training Accuracy:   1.0000 (100.00%)\n",
      "   Validation Accuracy: 0.9905 (99.05%)\n",
      "\n",
      "‚úì Model ready for inference!\n",
      "\n",
      "üìê Preparing sentence vectors for retrieval...\n",
      "\n",
      "ü§ñ Initializing chatbot...\n",
      "‚úì Chatbot ready!\n",
      "\n",
      "================================================================================\n",
      "üß™ TESTING INFERENCE\n",
      "================================================================================\n",
      "\n",
      "Running test queries:\n",
      "\n",
      "üë§ User: hello\n",
      "ü§ñ Bot: Hello! üëã Welcome to Chattea.\n",
      "\n",
      "I'm here to help you navigate features like sending messages, managing instances, filtering contacts, scheduling messages, and more.\n",
      "\n",
      "How can I assist you today?\n",
      "--------------------------------------------------------------------------------\n",
      "üë§ User: what is chattea\n",
      "ü§ñ Bot: Chattea is a WhatsApp marketing automation platform designed for businesses.\n",
      "\n",
      "üéØ Key features:\n",
      "‚Ä¢ Send mass messages (blast)\n",
      "‚Ä¢ Schedule messages\n",
      "‚Ä¢ Manage multiple WhatsApp instances\n",
      "‚Ä¢ Filter & validate ...\n",
      "--------------------------------------------------------------------------------\n",
      "üë§ User: how to blast message\n",
      "ü§ñ Bot: To send a message:\n",
      "\n",
      "1. Choose an active instance\n",
      "2. Enter the destination number (e.g., 628123456789)\n",
      "3. Type your message\n",
      "4. Click **Send**\n",
      "\n",
      "The message will be delivered immediately.\n",
      "\n",
      "üí° Tip: Use var...\n",
      "--------------------------------------------------------------------------------\n",
      "üë§ User: create instance\n",
      "ü§ñ Bot: To create a new WhatsApp instance:\n",
      "\n",
      "1. Open the **Instances** tab\n",
      "2. Click **+ New Instance**\n",
      "3. Enter a name (e.g., Marketing, Support, Sales)\n",
      "4. Scan the QR code with WhatsApp via:\n",
      "   Settings ‚Üí Lin...\n",
      "--------------------------------------------------------------------------------\n",
      "üë§ User: send bulk messages\n",
      "ü§ñ Bot: To send a message:\n",
      "\n",
      "1. Choose an active instance\n",
      "2. Enter the destination number (e.g., 628123456789)\n",
      "3. Type your message\n",
      "4. Click **Send**\n",
      "\n",
      "The message will be delivered immediately.\n",
      "\n",
      "üí° Tip: Use var...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "üìä MODEL EVALUATION\n",
      "================================================================================\n",
      "‚úì Validation Accuracy: 0.9905 (99.05%)\n",
      "\n",
      "üìã Per-Intent Performance:\n",
      "   contact                       : 1.000 (32 samples)\n",
      "   create_group                  : 1.000 (32 samples)\n",
      "   create_instance               : 1.000 (32 samples)\n",
      "   delete_group                  : 1.000 (32 samples)\n",
      "   delete_instance               : 1.000 (32 samples)\n",
      "   edit_group                    : 1.000 (32 samples)\n",
      "   edit_instance                 : 1.000 (32 samples)\n",
      "   filter_number                 : 1.000 (32 samples)\n",
      "   greeting                      : 0.846 (13 samples)\n",
      "   pricing                       : 1.000 (32 samples)\n",
      "   schedule_message              : 1.000 (32 samples)\n",
      "   send_message                  : 1.000 (32 samples)\n",
      "   unknown                       : 0.958 (24 samples)\n",
      "   what_for                      : 0.969 (32 samples)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ EVALUATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\nüìÇ Loading data...\")\n",
    "    if not os.path.exists(Config.DATASET_PATH):\n",
    "        raise FileNotFoundError(f\"Dataset not found: {Config.DATASET_PATH}\")\n",
    "\n",
    "    df = pd.read_csv(Config.DATASET_PATH)\n",
    "    if \"text\" not in df.columns or \"intent\" not in df.columns:\n",
    "        raise ValueError(\"Dataset must have 'text' and 'intent' columns\")\n",
    "\n",
    "    print(f\"‚úì Loaded {len(df)} samples, {df['intent'].nunique()} intents\")\n",
    "\n",
    "    # Load responses\n",
    "    if not os.path.exists(Config.RESPONSES_PATH):\n",
    "        raise FileNotFoundError(f\"Responses file not found: {Config.RESPONSES_PATH}\")\n",
    "\n",
    "    with open(Config.RESPONSES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        responses = json.load(f)\n",
    "\n",
    "    # Build vocabulary (for fuzzy)\n",
    "    print(\"\\nüìö Building vocabulary...\")\n",
    "    vocab = build_vocabulary(df['text'].tolist())\n",
    "    print(f\"‚úì Vocabulary: {len(vocab)} words\")\n",
    "\n",
    "    # Label encoding\n",
    "    print(\"\\nüè∑Ô∏è  Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    df['label'] = le.fit_transform(df['intent'].astype(str))\n",
    "    num_classes = len(le.classes_)\n",
    "    print(f\"‚úì Classes: {num_classes}\")\n",
    "\n",
    "    # Tokenize\n",
    "    print(\"\\n‚úÇÔ∏è  Tokenizing...\")\n",
    "    df['tokens'] = df['text'].apply(lambda t: tokenize(str(t)))\n",
    "\n",
    "    # Word2Vec\n",
    "    embedder = Word2VecEmbedder()\n",
    "    if os.path.exists(Config.WORD2VEC_PATH):\n",
    "        embedder.load(Config.WORD2VEC_PATH)\n",
    "    else:\n",
    "        embedder.train(df['tokens'].tolist())\n",
    "        embedder.save(Config.WORD2VEC_PATH)\n",
    "\n",
    "    # Prepare sequences\n",
    "    print(\"\\nüìä Preparing sequences...\")\n",
    "    sequences = np.array([embedder.encode_sequence(tokens, Config.MAX_SEQ_LENGTH) for tokens in df['tokens']], dtype=np.int64)\n",
    "\n",
    "    X = torch.tensor(sequences, dtype=torch.long)\n",
    "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "    # Train/val split\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(df)),\n",
    "        test_size=Config.TEST_SIZE,\n",
    "        random_state=Config.RANDOM_SEED,\n",
    "        stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    # Build model\n",
    "    model = TextCNN(vocab_size=embedder.vocab_size, embedding_dim=Config.EMBEDDING_DIM, num_classes=num_classes, embedding_matrix=embedder.embedding_matrix)\n",
    "\n",
    "    # Train or load model\n",
    "    if os.path.exists(Config.MODEL_PATH):\n",
    "        print(f\"\\n‚úì Found existing model: {Config.MODEL_PATH}\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(Config.MODEL_PATH, map_location=config.DEVICE))\n",
    "            model.eval()\n",
    "            print(\"‚úì Model loaded!\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load model, will retrain:\", e)\n",
    "            model = train_model_pretty(model, X_train, y_train, X_val, y_val)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No pre-trained model found. Training from scratch...\")\n",
    "        model = train_model_pretty(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Prepare sentence vectors for retrieval\n",
    "    print(\"\\nüìê Preparing sentence vectors for retrieval...\")\n",
    "    sent_vecs = np.stack([embedder.sentence_vector(tokens) for tokens in df['tokens']])\n",
    "    norms = np.linalg.norm(sent_vecs, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    sent_vecs_normalized = sent_vecs / norms\n",
    "\n",
    "    # Create bot\n",
    "    print(\"\\nü§ñ Initializing chatbot...\")\n",
    "    bot = ChatteaBot(model, embedder, le, responses, df, sent_vecs_normalized, vocab)\n",
    "    print(\"‚úì Chatbot ready!\")\n",
    "\n",
    "    # Inference tests\n",
    "    pretty_inference_tests(bot)\n",
    "\n",
    "    # Evaluation\n",
    "    pretty_evaluation(model, X_val, y_val, le, df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adb2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Deleted old model\n",
      "‚úì Cleared GPU cache\n",
      "Deleted Old Word2Vec Model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FRESH START - DELETE EVERYTHING AND RETRAIN\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# 1. Delete saved model\n",
    "if os.path.exists(\"chattea.pth\"):\n",
    "    os.remove(\"chattea.pth\")\n",
    "    print(\"‚úì Deleted old model\")\n",
    "\n",
    "# 2. Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úì Cleared GPU cache\")\n",
    "\n",
    "# 3. Delete saved word2vec.model\n",
    "if os.path.exists(\"word2vec.model\"):\n",
    "    os.remove(\"word2vec.model\")\n",
    "    print(\"Deleted Old Word2Vec Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
