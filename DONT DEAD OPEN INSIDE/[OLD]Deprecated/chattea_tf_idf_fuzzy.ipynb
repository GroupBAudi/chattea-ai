{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chattea Chatbot - TF-IDF + Cosine Similarity (From Scratch)\n",
    "# Educational AI Model for Customer Support\n",
    "\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad161c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and tokenize text\n",
    "    Args:\n",
    "        text (str): Raw input text\n",
    "    Returns:\n",
    "        list: List of tokens (words)\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters, keep spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize (split by whitespace)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove single character tokens (optional)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FUZZY MATCHING (LEVENSHTEIN DISTANCE)\n",
    "# ============================================================================\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate edit distance between two strings\n",
    "    Args:\n",
    "        s1, s2 (str): Two strings to compare\n",
    "    Returns:\n",
    "        int: Minimum number of edits needed\n",
    "    \"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    \n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    previous_row = range(len(s2) + 1)\n",
    "    \n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            # Cost of insertions, deletions, substitutions\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "\n",
    "def fuzzy_match(word, candidates, threshold=2):\n",
    "    \"\"\"\n",
    "    Find closest match for a word from candidates\n",
    "    Args:\n",
    "        word (str): Word to match\n",
    "        candidates (list): List of possible matches\n",
    "        threshold (int): Maximum edit distance allowed\n",
    "    Returns:\n",
    "        str or None: Best match or None\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    best_distance = float('inf')\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        distance = levenshtein_distance(word.lower(), candidate.lower())\n",
    "        if distance < best_distance and distance <= threshold:\n",
    "            best_distance = distance\n",
    "            best_match = candidate\n",
    "    \n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da676533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: TF-IDF FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "class TFIDFVectorizer:\n",
    "    \"\"\"\n",
    "    TF-IDF Vectorizer built from scratch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}  # word -> index mapping\n",
    "        self.idf = {}  # word -> IDF score\n",
    "        self.documents = []\n",
    "        \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Learn vocabulary and IDF scores from documents\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents (list of lists)\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        n_documents = len(documents)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        all_words = set()\n",
    "        for doc in documents:\n",
    "            all_words.update(doc)\n",
    "        \n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(all_words))}\n",
    "        \n",
    "        # Calculate IDF for each word\n",
    "        # IDF(word) = log(N / df(word))\n",
    "        # where N = total documents, df = document frequency\n",
    "        \n",
    "        document_frequency = Counter()\n",
    "        for doc in documents:\n",
    "            unique_words = set(doc)\n",
    "            for word in unique_words:\n",
    "                document_frequency[word] += 1\n",
    "        \n",
    "        for word in self.vocabulary:\n",
    "            df = document_frequency[word]\n",
    "            # Add smoothing to avoid division by zero\n",
    "            self.idf[word] = math.log((n_documents + 1) / (df + 1)) + 1\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Transform documents to TF-IDF vectors\n",
    "        Args:\n",
    "            documents (list): List of tokenized documents\n",
    "        Returns:\n",
    "            list: List of TF-IDF vectors (dictionaries)\n",
    "        \"\"\"\n",
    "        vectors = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            vector = defaultdict(float)\n",
    "            doc_length = len(doc)\n",
    "            \n",
    "            if doc_length == 0:\n",
    "                vectors.append(vector)\n",
    "                continue\n",
    "            \n",
    "            # Calculate term frequency for this document\n",
    "            term_freq = Counter(doc)\n",
    "            \n",
    "            # Calculate TF-IDF for each term\n",
    "            for word, count in term_freq.items():\n",
    "                if word in self.vocabulary:\n",
    "                    # TF = count / total_words_in_doc\n",
    "                    tf = count / doc_length\n",
    "                    # TF-IDF = TF * IDF\n",
    "                    vector[word] = tf * self.idf[word]\n",
    "            \n",
    "            vectors.append(vector)\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"\n",
    "        Fit and transform in one step\n",
    "        \"\"\"\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: COSINE SIMILARITY\n",
    "# ============================================================================\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors\n",
    "    Args:\n",
    "        vec1, vec2 (dict): TF-IDF vectors as dictionaries\n",
    "    Returns:\n",
    "        float: Similarity score between 0 and 1\n",
    "    \"\"\"\n",
    "    # Get common words\n",
    "    common_words = set(vec1.keys()) & set(vec2.keys())\n",
    "    \n",
    "    if not common_words:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate dot product\n",
    "    dot_product = sum(vec1[word] * vec2[word] for word in common_words)\n",
    "    \n",
    "    # Calculate magnitudes\n",
    "    magnitude1 = math.sqrt(sum(val ** 2 for val in vec1.values()))\n",
    "    magnitude2 = math.sqrt(sum(val ** 2 for val in vec2.values()))\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Cosine similarity\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c74fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: CHATBOT CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ChatteaChatbot:\n",
    "    \"\"\"\n",
    "    Main chatbot class using TF-IDF and Cosine Similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, intents_data, responses_data, confidence_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Initialize chatbot\n",
    "        Args:\n",
    "            intents_data (list): List of dicts with 'intent' and 'text' examples\n",
    "            responses_data (dict): Intent -> response mapping\n",
    "            confidence_threshold (float): Minimum similarity score to accept match\n",
    "        \"\"\"\n",
    "        self.intents_data = intents_data\n",
    "        self.responses_data = responses_data\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.vectorizer = TFIDFVectorizer()\n",
    "        \n",
    "        # Key terms for fuzzy matching (Chattea features)\n",
    "        self.key_terms = [\n",
    "            'blast', 'message', 'schedule', 'filter', 'whatsapp', \n",
    "            'number', 'panasin', 'chatbot', 'help', 'kirim', 'pesan'\n",
    "        ]\n",
    "        \n",
    "        # Prepare training data\n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare and vectorize training data\n",
    "        \"\"\"\n",
    "        # Extract intents and their example texts\n",
    "        self.intents = []\n",
    "        self.intent_texts = []\n",
    "        \n",
    "        for item in self.intents_data:\n",
    "            intent = item['intent']\n",
    "            text = item['text']\n",
    "            \n",
    "            self.intents.append(intent)\n",
    "            self.intent_texts.append(text)\n",
    "        \n",
    "        # Preprocess all training texts\n",
    "        tokenized_texts = [preprocess_text(text) for text in self.intent_texts]\n",
    "        \n",
    "        # Fit TF-IDF vectorizer\n",
    "        self.intent_vectors = self.vectorizer.fit_transform(tokenized_texts)\n",
    "        \n",
    "        print(f\"âœ“ Trained on {len(self.intents)} intent examples\")\n",
    "        print(f\"âœ“ Vocabulary size: {len(self.vectorizer.vocabulary)}\")\n",
    "    \n",
    "    def _apply_fuzzy_matching(self, tokens):\n",
    "        \"\"\"\n",
    "        Apply fuzzy matching to tokens to catch typos\n",
    "        \"\"\"\n",
    "        corrected_tokens = []\n",
    "        for token in tokens:\n",
    "            match = fuzzy_match(token, self.key_terms, threshold=2)\n",
    "            corrected_tokens.append(match if match else token)\n",
    "        return corrected_tokens\n",
    "    \n",
    "    def predict(self, user_input, top_k=3):\n",
    "        \"\"\"\n",
    "        Predict intent for user input\n",
    "        Args:\n",
    "            user_input (str): User's message\n",
    "            top_k (int): Return top K matches\n",
    "        Returns:\n",
    "            list: List of tuples (intent, confidence, response)\n",
    "        \"\"\"\n",
    "        # Preprocess user input\n",
    "        tokens = preprocess_text(user_input)\n",
    "        \n",
    "        # Apply fuzzy matching\n",
    "        tokens = self._apply_fuzzy_matching(tokens)\n",
    "        \n",
    "        # Vectorize user input\n",
    "        user_vector = self.vectorizer.transform([tokens])[0]\n",
    "        \n",
    "        # Calculate similarity with all intents\n",
    "        similarities = []\n",
    "        for i, intent_vector in enumerate(self.intent_vectors):\n",
    "            similarity = cosine_similarity(user_vector, intent_vector)\n",
    "            similarities.append((self.intents[i], similarity))\n",
    "        \n",
    "        # Sort by similarity (highest first)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top K results with responses\n",
    "        results = []\n",
    "        for intent, confidence in similarities[:top_k]:\n",
    "            if confidence >= self.confidence_threshold:\n",
    "                response = self.responses_data.get(intent, \"Maaf, saya tidak mengerti.\")\n",
    "                results.append((intent, confidence, response))\n",
    "        \n",
    "        # If no confident match, return fallback\n",
    "        if not results:\n",
    "            return [(\"unknown\", 0.0, \"Maaf, saya tidak mengerti pertanyaan Anda. Bisa tolong diperjelas?\")]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"\n",
    "        Get chatbot response for user input\n",
    "        Args:\n",
    "            user_input (str): User's message\n",
    "        Returns:\n",
    "            str: Bot's response\n",
    "        \"\"\"\n",
    "        results = self.predict(user_input, top_k=1)\n",
    "        intent, confidence, response = results[0]\n",
    "        \n",
    "        print(f\"\\n[DEBUG] Detected Intent: {intent} (confidence: {confidence:.3f})\")\n",
    "        \n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: DEMO & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "# Sample dataset for Chattea\n",
    "intents_dataset = [\n",
    "    # Blast message intents\n",
    "    {\"intent\": \"blast_message\", \"text\": \"cara blast message\"},\n",
    "    {\"intent\": \"blast_message\", \"text\": \"kirim pesan massal\"},\n",
    "    {\"intent\": \"blast_message\", \"text\": \"bagaimana mengirim blast\"},\n",
    "    {\"intent\": \"blast_message\", \"text\": \"blast pesan ke banyak orang\"},\n",
    "    \n",
    "    # Schedule message intents\n",
    "    {\"intent\": \"schedule_message\", \"text\": \"jadwalkan pesan\"},\n",
    "    {\"intent\": \"schedule_message\", \"text\": \"cara schedule message\"},\n",
    "    {\"intent\": \"schedule_message\", \"text\": \"kirim pesan terjadwal\"},\n",
    "    {\"intent\": \"schedule_message\", \"text\": \"atur waktu kirim pesan\"},\n",
    "    \n",
    "    # Filter WhatsApp number\n",
    "    {\"intent\": \"filter_number\", \"text\": \"filter nomor whatsapp\"},\n",
    "    {\"intent\": \"filter_number\", \"text\": \"cek nomor terdaftar\"},\n",
    "    {\"intent\": \"filter_number\", \"text\": \"validasi nomor whatsapp\"},\n",
    "    {\"intent\": \"filter_number\", \"text\": \"nomor aktif whatsapp\"},\n",
    "    \n",
    "    # Panasin WhatsApp\n",
    "    {\"intent\": \"panasin_wa\", \"text\": \"panasin whatsapp\"},\n",
    "    {\"intent\": \"panasin_wa\", \"text\": \"cara panasin wa\"},\n",
    "    {\"intent\": \"panasin_wa\", \"text\": \"hindari blokir whatsapp\"},\n",
    "    {\"intent\": \"panasin_wa\", \"text\": \"warming up whatsapp\"},\n",
    "    \n",
    "    # General help\n",
    "    {\"intent\": \"help\", \"text\": \"bantuan\"},\n",
    "    {\"intent\": \"help\", \"text\": \"tolong saya\"},\n",
    "    {\"intent\": \"help\", \"text\": \"help\"},\n",
    "    {\"intent\": \"help\", \"text\": \"apa yang bisa dilakukan\"},\n",
    "    \n",
    "    # Greeting\n",
    "    {\"intent\": \"greeting\", \"text\": \"halo\"},\n",
    "    {\"intent\": \"greeting\", \"text\": \"hi\"},\n",
    "    {\"intent\": \"greeting\", \"text\": \"hai\"},\n",
    "    {\"intent\": \"greeting\", \"text\": \"selamat pagi\"},\n",
    "]\n",
    "\n",
    "responses_dataset = {\n",
    "    \"blast_message\": \"Untuk blast message, buka menu 'Blast' â†’ Pilih kontak â†’ Tulis pesan â†’ Klik 'Kirim'. Anda bisa mengirim hingga 1000 pesan sekaligus!\",\n",
    "    \"schedule_message\": \"Untuk schedule message, buka 'Schedule' â†’ Pilih kontak â†’ Tulis pesan â†’ Atur tanggal & waktu â†’ Simpan. Pesan akan terkirim otomatis!\",\n",
    "    \"filter_number\": \"Fitur Filter Number membantu Anda memeriksa nomor mana yang terdaftar di WhatsApp. Buka 'Filter' â†’ Upload daftar nomor â†’ Sistem akan validasi secara otomatis.\",\n",
    "    \"panasin_wa\": \"Panasin WhatsApp adalah fitur untuk mengurangi risiko banned. Sistem membuat beberapa instance chat saling berkomunikasi. Aktifkan di menu 'Settings' â†’ 'Warming Up'.\",\n",
    "    \"help\": \"Saya bisa bantu Anda dengan:\\n1. Blast Message\\n2. Schedule Message\\n3. Filter WhatsApp Number\\n4. Panasin WhatsApp\\n\\nSilakan tanyakan fitur yang ingin Anda pelajari!\",\n",
    "    \"greeting\": \"Halo! Saya Chattea Bot. Saya siap membantu Anda menggunakan aplikasi Chattea. Ada yang bisa saya bantu?\",\n",
    "}\n",
    "\n",
    "# Initialize chatbot\n",
    "print(\"=\" * 60)\n",
    "print(\"CHATTEA CHATBOT - TF-IDF FROM SCRATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bot = ChatteaChatbot(intents_dataset, responses_dataset, confidence_threshold=0.25)\n",
    "\n",
    "# Test cases (including typos!)\n",
    "test_inputs = [\n",
    "    \"cara blst message\",  # typo: blst -> blast\n",
    "    \"jadwalkan pesan saya\",\n",
    "    \"gimana filter nomor wa\",\n",
    "    \"pnasin whatsapp\",  # typo: pnasin -> panasin\n",
    "    \"tolong bantu saya\",\n",
    "    \"halo bot\",\n",
    "    \"kirim pesam massal\",  # typo: pesam -> pesan\n",
    "    \"schedule mesage ke customer\",  # typo: mesage -> message\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING CHATBOT WITH TYPOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for test_input in test_inputs:\n",
    "    print(f\"\\nðŸ‘¤ User: {test_input}\")\n",
    "    response = bot.chat(test_input)\n",
    "    print(f\"ðŸ¤– Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"INTERACTIVE MODE (type 'quit' to exit)\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"\\nðŸ‘¤ You: \")\n",
    "#     if user_input.lower() in ['quit', 'exit', 'keluar']:\n",
    "#         print(\"ðŸ‘‹ Terima kasih! Sampai jumpa!\")\n",
    "#         break\n",
    "    \n",
    "#     response = bot.chat(user_input)\n",
    "#     print(f\"ðŸ¤– Bot: {response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
