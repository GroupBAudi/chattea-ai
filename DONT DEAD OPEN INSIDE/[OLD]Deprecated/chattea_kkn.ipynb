{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chattea Intent Classifier - KNN with Smart Intent Grouping\n",
    "\n",
    "\"\"\"\n",
    "INSTALLATION:\n",
    "pip install sentence-transformers scikit-learn pandas numpy matplotlib seaborn langdetect\n",
    "\n",
    "DATASET REQUIREMENTS:\n",
    "- chattea.csv (your 430 samples)\n",
    "- responses_bilingual.json (your 85 intent responses)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "try:\n",
    "    from langdetect import detect\n",
    "    LANGDETECT_AVAILABLE = True\n",
    "except:\n",
    "    LANGDETECT_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  langdetect not available - using fallback language detection\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHATTEA KNN INTENT CLASSIFIER - OPTIMIZED FOR YOUR DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Strategy: Smart Intent Grouping (85 ‚Üí 25 classes)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths (adjust if needed)\n",
    "    DATA_CSV = \"chattea.csv\"\n",
    "    RESPONSES_JSON = \"responses_bilingual.json\"\n",
    "    OUTPUT_DIR = \"artifacts_knn\"\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    \n",
    "    # KNN parameters (optimized for small dataset)\n",
    "    N_NEIGHBORS = 3  # Lower for small data\n",
    "    METRIC = 'cosine'\n",
    "    WEIGHTS = 'distance'\n",
    "    \n",
    "    # Data split\n",
    "    TEST_SIZE = 0.20\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # Grouping strategy\n",
    "    USE_GROUPING = True\n",
    "\n",
    "config = Config()\n",
    "Path(config.OUTPUT_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: CUSTOM INTENT GROUPING (Based on YOUR 85 intents)\n",
    "# ============================================================================\n",
    "\n",
    "# Analyzed YOUR actual intents and created optimal groups!\n",
    "INTENT_GROUPS = {\n",
    "    # Core user actions (keep separate - important!)\n",
    "    'greeting': ['greeting'],\n",
    "    'goodbye': ['goodbye'],\n",
    "    'gratitude': ['gratitude'],\n",
    "    'help': ['support_contact'],\n",
    "    'cancel': ['cancel_action'],\n",
    "    \n",
    "    # Account & Auth (2 ‚Üí 1)\n",
    "    'account': [\n",
    "        'account_setup',\n",
    "        'login_issue'\n",
    "    ],\n",
    "    \n",
    "    # Definition & Info (3 ‚Üí 1)\n",
    "    'info': [\n",
    "        'definition',\n",
    "        'feature_coming_soon',\n",
    "        'out_of_scope'  # Map unknown to info request\n",
    "    ],\n",
    "    \n",
    "    # Navigation - All tabs (8 ‚Üí 1)\n",
    "    'navigation': [\n",
    "        'navigation_main_dashboard',\n",
    "        'navigation_instances_tab',\n",
    "        'navigation_chat_tab',\n",
    "        'navigation_grouping_tab',\n",
    "        'navigation_files_tab',\n",
    "        'navigation_tasks_tab',\n",
    "        'navigation_payments_tab'\n",
    "    ],\n",
    "    \n",
    "    # Instance Management (8 ‚Üí 1)\n",
    "    'instance': [\n",
    "        'create_instance',\n",
    "        'function_create_instance',\n",
    "        'instance_edit',\n",
    "        'instance_list',\n",
    "        'instance_connection_status',\n",
    "        'instance_logout',\n",
    "        'instance_delete',\n",
    "        'instance_view_profile'\n",
    "    ],\n",
    "    \n",
    "    # Pairing/Connection (2 ‚Üí 1)\n",
    "    'pairing': [\n",
    "        'pairing',\n",
    "        'troubleshoot_qr'\n",
    "    ],\n",
    "    \n",
    "    # Chat & Messaging (4 ‚Üí 1)\n",
    "    'chat': [\n",
    "        'chat_send',\n",
    "        'chat_read',\n",
    "        'chat_history',\n",
    "        'function_send_message'\n",
    "    ],\n",
    "    \n",
    "    # Contacts (4 ‚Üí 1)\n",
    "    'contacts': [\n",
    "        'contacts_manage',\n",
    "        'contacts_add',\n",
    "        'contacts_filter',\n",
    "        'advanced_contact_segmentation'\n",
    "    ],\n",
    "    \n",
    "    # Groups (4 ‚Üí 1)\n",
    "    'groups': [\n",
    "        'group_create',\n",
    "        'function_create_group',\n",
    "        'group_export',\n",
    "        'group_settings_edit'\n",
    "    ],\n",
    "    \n",
    "    # Files (3 ‚Üí 1)\n",
    "    'files': [\n",
    "        'files_upload',\n",
    "        'files_manage',\n",
    "        'files_share'\n",
    "    ],\n",
    "    \n",
    "    # Tasks (3 ‚Üí 1)\n",
    "    'tasks': [\n",
    "        'tasks_view',\n",
    "        'tasks_monitor',\n",
    "        'tasks_view_failed'\n",
    "    ],\n",
    "    \n",
    "    # Payment (8 ‚Üí 1)\n",
    "    'payment': [\n",
    "        'payment_subscribe',\n",
    "        'payment_upgrade',\n",
    "        'payment_status',\n",
    "        'payment_history',\n",
    "        'payment_methods',\n",
    "        'payment_issue',\n",
    "        'payment_extend'\n",
    "    ],\n",
    "    \n",
    "    # Pricing (3 ‚Üí 1)\n",
    "    'pricing': [\n",
    "        'pricing_query',\n",
    "        'pricing_currency_idr',\n",
    "        'pricing_instance_limits'\n",
    "    ],\n",
    "    \n",
    "    # Messaging Operations (6 ‚Üí 1)\n",
    "    'messaging': [\n",
    "        'message_blast',\n",
    "        'message_blast_status',\n",
    "        'message_schedule',\n",
    "        'message_schedule_recurring',\n",
    "        'message_schedule_timezone'\n",
    "    ],\n",
    "    \n",
    "    # Warmup (2 ‚Üí 1)\n",
    "    'warmup': [\n",
    "        'warmup_enable',\n",
    "        'warmup_info'\n",
    "    ],\n",
    "    \n",
    "    # API (3 ‚Üí 1)\n",
    "    'api': [\n",
    "        'api_reference',\n",
    "        'api_send_message',\n",
    "        'api_webhook_setup'\n",
    "    ],\n",
    "    \n",
    "    # Templates (2 ‚Üí 1)\n",
    "    'templates': [\n",
    "        'templates_create',\n",
    "        'templates_use'\n",
    "    ],\n",
    "    \n",
    "    # Platform (5 ‚Üí 1)\n",
    "    'platform': [\n",
    "        'platform_compare',\n",
    "        'platform_cloud_open',\n",
    "        'platform_desktop_install',\n",
    "        'platform_desktop_info',\n",
    "        'platform_updates'\n",
    "    ],\n",
    "    \n",
    "    # Troubleshooting (2 ‚Üí 1)\n",
    "    'troubleshoot': [\n",
    "        'troubleshoot_connection',\n",
    "        'troubleshoot_qr'  # Also in pairing - will use first match\n",
    "    ],\n",
    "    \n",
    "    # Security (3 ‚Üí 1)\n",
    "    'security': [\n",
    "        'security_privacy',\n",
    "        'security_payment',\n",
    "        'security_cloud_data'\n",
    "    ],\n",
    "    \n",
    "    # Tips & Optimization (3 ‚Üí 1)\n",
    "    'tips': [\n",
    "        'general_tips',\n",
    "        'tips_browser',\n",
    "        'tips_internet'\n",
    "    ],\n",
    "    \n",
    "    # Special Features (5 ‚Üí keep separate, important)\n",
    "    'phone_checker': ['function_check_phone'],\n",
    "    'analytics': ['analytics_view'],\n",
    "    'calendar': ['calendar_integration'],\n",
    "    'auto_reply': ['auto_reply_setup', 'auto_reply_disable'],\n",
    "    'quota': ['quota_reached'],\n",
    "}\n",
    "\n",
    "def create_intent_mapping(groups):\n",
    "    \"\"\"Create original_intent ‚Üí group_name mapping\"\"\"\n",
    "    intent_map = {}\n",
    "    for group_name, intents in groups.items():\n",
    "        for intent in intents:\n",
    "            intent_map[intent] = group_name\n",
    "    return intent_map\n",
    "\n",
    "def merge_intents_in_df(df, intent_mapping):\n",
    "    \"\"\"Apply intent grouping to dataframe\"\"\"\n",
    "    df = df.copy()\n",
    "    df['intent_original'] = df['intent']\n",
    "    df['intent_grouped'] = df['intent'].map(intent_mapping)\n",
    "    \n",
    "    # Check for unmapped intents\n",
    "    unmapped = df[df['intent_grouped'].isna()]\n",
    "    if len(unmapped) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: {len(unmapped)} samples have unmapped intents:\")\n",
    "        print(unmapped['intent'].unique())\n",
    "        # Map unmapped to 'info' as fallback\n",
    "        df['intent_grouped'] = df['intent_grouped'].fillna('info')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_grouped_responses(original_responses, intent_mapping):\n",
    "    \"\"\"Map grouped intents to their responses\"\"\"\n",
    "    grouped_responses = {}\n",
    "    \n",
    "    for original_intent, group_name in intent_mapping.items():\n",
    "        if original_intent in original_responses:\n",
    "            # Use the first response found for this group\n",
    "            if group_name not in grouped_responses:\n",
    "                grouped_responses[group_name] = original_responses[original_intent]\n",
    "    \n",
    "    return grouped_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb90f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare data\"\"\"\n",
    "    print(f\"\\nüìÇ Loading data...\")\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(config.DATA_CSV)\n",
    "    print(f\"‚úì Loaded {len(df)} samples from CSV\")\n",
    "    print(f\"‚úì Original intents: {df['intent'].nunique()}\")\n",
    "    \n",
    "    # Load responses\n",
    "    with open(config.RESPONSES_JSON, 'r', encoding='utf-8') as f:\n",
    "        responses = json.load(f)\n",
    "    print(f\"‚úì Loaded responses for {len(responses)} intents\")\n",
    "    \n",
    "    return df, responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36101843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_data(df, title=\"Dataset Analysis\"):\n",
    "    \"\"\"Comprehensive data analysis\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTotal samples: {len(df)}\")\n",
    "    print(f\"Unique intents: {df['intent'].nunique()}\")\n",
    "    print(f\"Avg samples per intent: {len(df) / df['intent'].nunique():.1f}\")\n",
    "    \n",
    "    # Intent distribution\n",
    "    intent_counts = df['intent'].value_counts()\n",
    "    \n",
    "    print(f\"\\nIntent Distribution Stats:\")\n",
    "    print(f\"  Max: {intent_counts.max()} samples\")\n",
    "    print(f\"  Min: {intent_counts.min()} samples\")\n",
    "    print(f\"  Median: {intent_counts.median():.0f} samples\")\n",
    "    print(f\"  Mean: {intent_counts.mean():.1f} samples\")\n",
    "    \n",
    "    print(f\"\\nIntents with < 10 samples:\")\n",
    "    low_sample_intents = intent_counts[intent_counts < 10]\n",
    "    print(f\"  Count: {len(low_sample_intents)} intents\")\n",
    "    if len(low_sample_intents) > 0:\n",
    "        print(f\"  Examples: {list(low_sample_intents.head(10).index)}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Top 20 intents\n",
    "    intent_counts.head(20).plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_title('Top 20 Intent Distribution')\n",
    "    axes[0].set_xlabel('Sample Count')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Sample distribution histogram\n",
    "    axes[1].hist(intent_counts.values, bins=20, color='coral', edgecolor='black')\n",
    "    axes[1].set_title('Samples per Intent Distribution')\n",
    "    axes[1].set_xlabel('Number of Samples')\n",
    "    axes[1].set_ylabel('Number of Intents')\n",
    "    axes[1].axvline(intent_counts.mean(), color='red', linestyle='--', label=f'Mean: {intent_counts.mean():.1f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{config.OUTPUT_DIR}/data_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n‚úì Saved: {config.OUTPUT_DIR}/data_analysis.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return intent_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2faf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: KNN CLASSIFIER\n",
    "# ============================================================================\n",
    "\n",
    "class KNNIntentClassifier:\n",
    "    \"\"\"KNN classifier using sentence embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, n_neighbors=3, metric='cosine', weights='distance'):\n",
    "        print(\"\\nüîß Initializing KNN Classifier...\")\n",
    "        print(f\"   Model: {model_name}\")\n",
    "        print(f\"   K-Neighbors: {n_neighbors}\")\n",
    "        print(f\"   Metric: {metric}\")\n",
    "        print(f\"   Weights: {weights}\")\n",
    "        \n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.knn = KNeighborsClassifier(\n",
    "            n_neighbors=n_neighbors,\n",
    "            metric=metric,\n",
    "            weights=weights\n",
    "        )\n",
    "        \n",
    "        self.train_embeddings = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        print(\"‚úì Classifier ready\")\n",
    "    \n",
    "    def fit(self, texts, labels):\n",
    "        \"\"\"Train the classifier\"\"\"\n",
    "        print(f\"\\nüìö Training on {len(texts)} samples...\")\n",
    "        \n",
    "        # Encode texts\n",
    "        print(\"   Encoding texts...\")\n",
    "        self.train_embeddings = self.embedder.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # Fit KNN\n",
    "        print(\"   Fitting KNN...\")\n",
    "        self.knn.fit(self.train_embeddings, labels)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        print(f\"‚úì Training complete! Classes: {len(self.knn.classes_)}\")\n",
    "        return self\n",
    "    \n",
    "    def predict_single(self, text):\n",
    "        \"\"\"Predict single query with confidence\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Classifier not trained yet!\")\n",
    "        \n",
    "        # Encode\n",
    "        embedding = self.embedder.encode([text], convert_to_numpy=True)\n",
    "        \n",
    "        # Predict\n",
    "        intent = self.knn.predict(embedding)[0]\n",
    "        proba = self.knn.predict_proba(embedding)[0]\n",
    "        confidence = proba.max()\n",
    "        \n",
    "        return intent, confidence\n",
    "    \n",
    "    def predict_batch(self, texts):\n",
    "        \"\"\"Predict multiple texts\"\"\"\n",
    "        embeddings = self.embedder.encode(\n",
    "            texts,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        predictions = self.knn.predict(embeddings)\n",
    "        probas = self.knn.predict_proba(embeddings)\n",
    "        confidences = probas.max(axis=1)\n",
    "        \n",
    "        return predictions, confidences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_classifier(classifier, test_texts, test_labels):\n",
    "    \"\"\"Evaluate with metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Predict\n",
    "    predictions, confidences = classifier.predict_batch(test_texts)\n",
    "    \n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        test_labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà Overall Metrics:\")\n",
    "    print(f\"   Accuracy:  {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "    print(f\"   Precision: {precision:.3f}\")\n",
    "    print(f\"   Recall:    {recall:.3f}\")\n",
    "    print(f\"   F1 Score:  {f1:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüíØ Confidence Statistics:\")\n",
    "    print(f\"   Mean:      {confidences.mean():.3f}\")\n",
    "    print(f\"   Median:    {np.median(confidences):.3f}\")\n",
    "    print(f\"   Min:       {confidences.min():.3f}\")\n",
    "    print(f\"   Max:       {confidences.max():.3f}\")\n",
    "    print(f\"   > 0.5:     {(confidences > 0.5).sum()} / {len(confidences)} ({(confidences > 0.5).mean()*100:.1f}%)\")\n",
    "    print(f\"   > 0.7:     {(confidences > 0.7).sum()} / {len(confidences)} ({(confidences > 0.7).mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nüìã Per-Class Performance:\")\n",
    "    print(classification_report(test_labels, predictions, zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    unique_labels = sorted(set(test_labels) | set(predictions))\n",
    "    \n",
    "    if len(unique_labels) <= 30:  # Only plot if reasonable size\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=unique_labels,\n",
    "            yticklabels=unique_labels,\n",
    "            cbar_kws={'label': 'Count'}\n",
    "        )\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{config.OUTPUT_DIR}/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Saved confusion matrix: {config.OUTPUT_DIR}/confusion_matrix.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'mean_confidence': confidences.mean(),\n",
    "        'median_confidence': np.median(confidences)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: CHATBOT INTERFACE\n",
    "# ============================================================================\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Simple language detection\"\"\"\n",
    "    if LANGDETECT_AVAILABLE:\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "            return \"id\" if lang in [\"id\", \"ms\"] else \"en\"\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Fallback: simple keyword matching\n",
    "    id_keywords = {'cara', 'kirim', 'pesan', 'jadwal', 'nomor', 'cek', 'tolong', 'saya', 'bisa', 'gimana'}\n",
    "    tokens = set(text.lower().split())\n",
    "    \n",
    "    if len(tokens & id_keywords) >= 1:\n",
    "        return \"id\"\n",
    "    return \"en\"\n",
    "\n",
    "def get_response(intent, responses_dict, language):\n",
    "    \"\"\"Get bilingual response\"\"\"\n",
    "    if intent not in responses_dict:\n",
    "        return \"Sorry, I don't have a response for that.\" if language == \"en\" else \"Maaf, saya tidak punya jawaban untuk itu.\"\n",
    "    \n",
    "    response_entry = responses_dict[intent]\n",
    "    \n",
    "    if isinstance(response_entry, dict):\n",
    "        return response_entry.get(language, response_entry.get('en', response_entry.get('id', '')))\n",
    "    \n",
    "    return str(response_entry)\n",
    "\n",
    "class ChatteaChatbot:\n",
    "    \"\"\"Production chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self, classifier, responses_dict):\n",
    "        self.classifier = classifier\n",
    "        self.responses = responses_dict\n",
    "        print(\"\\nü§ñ Chatbot initialized and ready!\")\n",
    "    \n",
    "    def chat(self, user_input, verbose=True):\n",
    "        \"\"\"Process user query\"\"\"\n",
    "        # Predict intent\n",
    "        intent, confidence = self.classifier.predict_single(user_input)\n",
    "        \n",
    "        # Detect language\n",
    "        language = detect_language(user_input)\n",
    "        \n",
    "        # Get response\n",
    "        response = get_response(intent, self.responses, language)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüéØ Intent: {intent}\")\n",
    "            print(f\"üíØ Confidence: {confidence:.3f}\")\n",
    "            print(f\"üåç Language: {language}\")\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'confidence': confidence,\n",
    "            'language': language,\n",
    "            'response': response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Complete pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 1: LOAD DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df, responses = load_data()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: ANALYZE ORIGINAL DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    analyze_data(df, \"Original Dataset (85 Intents)\")\n",
    "    \n",
    "    if config.USE_GROUPING:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: APPLY INTENT GROUPING\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        intent_mapping = create_intent_mapping(INTENT_GROUPS)\n",
    "        df = merge_intents_in_df(df, intent_mapping)\n",
    "        \n",
    "        print(f\"‚úì Grouped: 85 ‚Üí {df['intent_grouped'].nunique()} intents\")\n",
    "        print(f\"‚úì New avg samples per intent: {len(df) / df['intent_grouped'].nunique():.1f}\")\n",
    "        \n",
    "        # Use grouped intents\n",
    "        df['intent'] = df['intent_grouped']\n",
    "        \n",
    "        # Create grouped responses\n",
    "        grouped_responses = create_grouped_responses(responses, intent_mapping)\n",
    "        print(f\"‚úì Mapped {len(grouped_responses)} grouped responses\")\n",
    "        \n",
    "        responses = grouped_responses\n",
    "        \n",
    "        analyze_data(df, \"After Grouping\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: SPLIT DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=config.TEST_SIZE,\n",
    "        stratify=df['intent'],\n",
    "        random_state=config.RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"   Train: {len(train_df)} samples\")\n",
    "    print(f\"   Test:  {len(test_df)} samples\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: TRAIN CLASSIFIER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    classifier = KNNIntentClassifier(\n",
    "        model_name=config.MODEL_NAME,\n",
    "        n_neighbors=config.N_NEIGHBORS,\n",
    "        metric=config.METRIC,\n",
    "        weights=config.WEIGHTS\n",
    "    )\n",
    "    \n",
    "    classifier.fit(train_df['text'].tolist(), train_df['intent'].tolist())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 6: EVALUATE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    metrics = evaluate_classifier(\n",
    "        classifier,\n",
    "        test_df['text'].tolist(),\n",
    "        test_df['intent'].tolist()\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 7: DEMO CHATBOT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    bot = ChatteaChatbot(classifier, responses)\n",
    "    \n",
    "    demo_queries = [\n",
    "        \"What is Chattea?\",\n",
    "        \"gimana cara blast message?\",\n",
    "        \"how to schedule messages?\",\n",
    "        \"cek nomor wa yang valid\",\n",
    "        \"tolong bantu saya\",\n",
    "        \"what is the pricing plan?\",\n",
    "        \"create a new instance\",\n",
    "        \"pair with QR code\",\n",
    "        \"show me analytics\",\n",
    "        \"hello\",\n",
    "    ]\n",
    "    \n",
    "    for query in demo_queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üë§ User: {query}\")\n",
    "        result = bot.chat(query, verbose=True)\n",
    "        print(f\"ü§ñ Bot: {result['response'][:150]}{'...' if len(result['response']) > 150 else ''}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFinal Metrics:\")\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}\")\n",
    "    print(f\"   F1 Score: {metrics['f1']:.3f}\")\n",
    "    print(f\"   Avg Confidence: {metrics['mean_confidence']:.3f}\")\n",
    "    print(f\"   Intents: {df['intent'].nunique()} classes\")\n",
    "    print(f\"\\nArtifacts saved to: {config.OUTPUT_DIR}/\")\n",
    "    \n",
    "    return classifier, bot, metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier, bot, metrics = main()\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INTERACTIVE MODE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Type your questions (or 'quit' to exit)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nüë§ You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q', '']:\n",
    "                print(\"\\nüëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            result = bot.chat(user_input, verbose=False)\n",
    "            print(f\"üéØ {result['intent']} ({result['confidence']:.2f})\")\n",
    "            print(f\"ü§ñ {result['response']}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
