{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df93de17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_miniLM_faiss.py\\nMid-sized MiniLM intent classifier + FAISS fallback.\\n\\nUsage:\\n    python train_miniLM_faiss.py\\n\\nFiles expected (local):\\n- /mnt/data/unified_dataset.json   (preferred)\\n- /mnt/data/chattea.csv            (fallback)\\n- /mnt/data/responses_bilingual.json\\n\\nOutputs (saved under artifacts/):\\n- artifacts/intent_model/          (HuggingFace model + tokenizer)\\n- artifacts/faiss.index            (FAISS index file)\\n- artifacts/embeddings.npy         (embeddings matrix)\\n- artifacts/texts_intents.json     (index -> text/intent mapping)\\n- artifacts/label_mappings.json\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_miniLM_faiss.py\n",
    "Mid-sized MiniLM intent classifier + FAISS fallback.\n",
    "\n",
    "Usage:\n",
    "    python train_miniLM_faiss.py\n",
    "\n",
    "Files expected (local):\n",
    "- /mnt/data/unified_dataset.json   (preferred)\n",
    "- /mnt/data/chattea.csv            (fallback)\n",
    "- /mnt/data/responses_bilingual.json\n",
    "\n",
    "Outputs (saved under artifacts/):\n",
    "- artifacts/intent_model/          (HuggingFace model + tokenizer)\n",
    "- artifacts/faiss.index            (FAISS index file)\n",
    "- artifacts/embeddings.npy         (embeddings matrix)\n",
    "- artifacts/texts_intents.json     (index -> text/intent mapping)\n",
    "- artifacts/label_mappings.json\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ffd1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\Documents\\GitHub_Repository\\Python\\chattea-ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda | FAISS available: False | langdetect: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "\n",
    "# Optional: langdetect\n",
    "try:\n",
    "    from langdetect import detect\n",
    "    LANGDETECT_AVAILABLE = True\n",
    "except Exception:\n",
    "    LANGDETECT_AVAILABLE = False\n",
    "\n",
    "# Optional: FAISS\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except Exception:\n",
    "    FAISS_AVAILABLE = False\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "# Prefer unified dataset if present, otherwise fallback to uploaded CSV\n",
    "UNIFIED_PATH = Path(\"dataset.json\")\n",
    "CSV_PATH = Path(\"chattea_original.csv\")\n",
    "BILINGUAL_RESP_PATH = Path(\"responses_bilingual.json\")\n",
    "\n",
    "ARTIFACTS = BASE_DIR / \"artifacts\"\n",
    "ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "OUTPUT_MODEL_DIR = ARTIFACTS / \"intent_model\"\n",
    "EMBEDDINGS_PATH = ARTIFACTS / \"embeddings.npy\"\n",
    "FAISS_INDEX_PATH = ARTIFACTS / \"faiss.index\"\n",
    "TEXTS_INTENTS_PATH = ARTIFACTS / \"texts_intents.json\"\n",
    "LABEL_MAP_PATH = ARTIFACTS / \"label_mappings.json\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LENGTH = 128\n",
    "FP16 = True  # use mixed precision if GPU available\n",
    "CONFIDENCE_THRESHOLD = 0.6  # if classifier confidence < this, fallback to FAISS\n",
    "TOP_K_FAISS = 5\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"DEVICE: {DEVICE} | FAISS available: {FAISS_AVAILABLE} | langdetect: {LANGDETECT_AVAILABLE}\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"chattea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbc251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# UTILITIES\n",
    "# ----------------------------------------\n",
    "def load_unified_or_csv():\n",
    "    \"\"\"\n",
    "    Load dataset from unified json (preferred) or fallback to csv.\n",
    "    Expects fields: text, intent\n",
    "    \"\"\"\n",
    "    if UNIFIED_PATH.exists():\n",
    "        print(f\"Loading unified dataset: {UNIFIED_PATH}\")\n",
    "        with open(UNIFIED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.DataFrame([{\"text\": item[\"text\"], \"intent\": item[\"intent\"]} for item in data])\n",
    "    elif CSV_PATH.exists():\n",
    "        print(f\"Loading CSV dataset: {CSV_PATH}\")\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        # Expect columns 'text' and 'intent'\n",
    "        if \"text\" not in df.columns or \"intent\" not in df.columns:\n",
    "            raise ValueError(\"CSV must contain 'text' and 'intent' columns\")\n",
    "        df = df[[\"text\", \"intent\"]].copy()\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No dataset file found. Place dataset.json or chattea.csv in /mnt/data/\")\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "    df = df[df[\"text\"].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"Loaded {len(df)} samples, unique intents: {df['intent'].nunique()}\")\n",
    "    return df\n",
    "\n",
    "def load_bilingual_responses():\n",
    "    if not BILINGUAL_RESP_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Bilingual responses file missing: {BILINGUAL_RESP_PATH}\")\n",
    "    with open(BILINGUAL_RESP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        responses_bilingual = json.load(f)\n",
    "    print(f\"Loaded bilingual responses for {len(responses_bilingual)} intents\")\n",
    "    return responses_bilingual\n",
    "\n",
    "def preprocess_text(text):\n",
    "    t = str(text).lower()\n",
    "    t = \" \".join(t.split())\n",
    "    return t\n",
    "\n",
    "def detect_language_simple(text):\n",
    "    if LANGDETECT_AVAILABLE:\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "            if lang and (lang.startswith(\"id\") or lang == \"ms\"):\n",
    "                return \"id\"\n",
    "            else:\n",
    "                return \"en\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    # simple heuristic fallback\n",
    "    id_words = {\"cara\",\"kirim\",\"pesan\",\"jadwal\",\"nomor\",\"cek\",\"bantuan\",\"filter\",\"panasin\",\"harga\"}\n",
    "    tokens = text.lower().split()\n",
    "    if len(tokens) == 0:\n",
    "        return \"en\"\n",
    "    id_count = sum(1 for t in tokens if t in id_words)\n",
    "    if id_count >= max(1, int(0.3 * len(tokens))):\n",
    "        return \"id\"\n",
    "    return \"en\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67ed5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unified dataset: dataset.json\n",
      "Loaded 2469 samples, unique intents: 85\n",
      "Loaded bilingual responses for 85 intents\n",
      "Label mapping sample: [('account_setup', 0), ('advanced_contact_segmentation', 1), ('analytics_view', 2), ('api_reference', 3), ('api_send_message', 4), ('api_webhook_setup', 5), ('auto_reply_disable', 6), ('auto_reply_setup', 7), ('calendar_integration', 8), ('cancel_action', 9)]\n",
      "Train: 1888, Val: 210, Test: 371\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# PREPARE DATA\n",
    "# ----------------------------------------\n",
    "df = load_unified_or_csv()\n",
    "responses_bilingual = load_bilingual_responses()\n",
    "\n",
    "# Build label mappings\n",
    "unique_intents = sorted(df[\"intent\"].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(unique_intents)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "df[\"label\"] = df[\"intent\"].map(label2id)\n",
    "print(\"Label mapping sample:\", list(label2id.items())[:10])\n",
    "\n",
    "# Save mapping\n",
    "with open(LABEL_MAP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"label2id\": label2id, \"id2label\": id2label}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Train/val/test split (stratified)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.15, stratify=df[\"label\"], random_state=SEED)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.10, stratify=train_val_df[\"label\"], random_state=SEED)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Create HF DatasetDict\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
    "val_ds = Dataset.from_pandas(val_df[[\"text\", \"label\"]])\n",
    "test_ds = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
    "dataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e8bb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1888/1888 [00:00<00:00, 8367.49 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [00:00<00:00, 7611.44 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 371/371 [00:00<00:00, 8350.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# TOKENIZER & MODEL\n",
    "# ----------------------------------------\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b55d7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_11496\\4059153.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 0, 'pad_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='708' max='708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [708/708 01:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.153900</td>\n",
       "      <td>4.014304</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.660658</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.638594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.864300</td>\n",
       "      <td>3.762574</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.794864</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.752188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.746500</td>\n",
       "      <td>3.682552</td>\n",
       "      <td>0.804762</td>\n",
       "      <td>0.833515</td>\n",
       "      <td>0.804762</td>\n",
       "      <td>0.784943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to: c:\\Users\\nicho\\Documents\\GitHub_Repository\\Python\\chattea-ai\\artifacts\\intent_model\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report on TEST:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                account_setup       1.00      0.25      0.40         4\n",
      "advanced_contact_segmentation       1.00      0.80      0.89         5\n",
      "               analytics_view       1.00      1.00      1.00         5\n",
      "                api_reference       0.83      1.00      0.91         5\n",
      "             api_send_message       1.00      1.00      1.00         2\n",
      "            api_webhook_setup       1.00      0.75      0.86         4\n",
      "           auto_reply_disable       0.67      1.00      0.80         4\n",
      "             auto_reply_setup       1.00      1.00      1.00         5\n",
      "         calendar_integration       1.00      1.00      1.00         4\n",
      "                cancel_action       0.00      0.00      0.00         3\n",
      "                 chat_history       1.00      1.00      1.00         5\n",
      "                    chat_read       1.00      1.00      1.00         2\n",
      "                    chat_send       0.50      1.00      0.67         4\n",
      "                 contacts_add       0.67      1.00      0.80         4\n",
      "              contacts_filter       0.67      0.40      0.50         5\n",
      "              contacts_manage       0.50      1.00      0.67         4\n",
      "              create_instance       0.57      0.80      0.67         5\n",
      "                   definition       0.83      1.00      0.91         5\n",
      "          feature_coming_soon       1.00      1.00      1.00         5\n",
      "                 files_manage       0.00      0.00      0.00         2\n",
      "                  files_share       1.00      0.50      0.67         4\n",
      "                 files_upload       0.83      1.00      0.91         5\n",
      "         function_check_phone       1.00      1.00      1.00         4\n",
      "        function_create_group       0.33      1.00      0.50         4\n",
      "     function_create_instance       0.75      0.60      0.67         5\n",
      "        function_send_message       1.00      0.25      0.40         4\n",
      "                 general_tips       0.83      1.00      0.91         5\n",
      "                      goodbye       1.00      0.75      0.86         4\n",
      "                    gratitude       1.00      0.50      0.67         4\n",
      "                     greeting       0.67      0.50      0.57         4\n",
      "                 group_create       1.00      0.60      0.75         5\n",
      "                 group_export       1.00      0.40      0.57         5\n",
      "          group_settings_edit       1.00      0.40      0.57         5\n",
      "   instance_connection_status       1.00      0.80      0.89         5\n",
      "              instance_delete       0.83      1.00      0.91         5\n",
      "                instance_edit       1.00      1.00      1.00         4\n",
      "                instance_list       0.38      1.00      0.56         5\n",
      "              instance_logout       1.00      0.50      0.67         4\n",
      "        instance_view_profile       1.00      1.00      1.00         4\n",
      "                  login_issue       0.83      1.00      0.91         5\n",
      "                message_blast       0.67      0.50      0.57         4\n",
      "         message_blast_status       0.67      1.00      0.80         4\n",
      "             message_schedule       0.80      1.00      0.89         4\n",
      "   message_schedule_recurring       1.00      0.60      0.75         5\n",
      "    message_schedule_timezone       1.00      1.00      1.00         5\n",
      "          navigation_chat_tab       1.00      1.00      1.00         5\n",
      "         navigation_files_tab       0.57      1.00      0.73         4\n",
      "      navigation_grouping_tab       1.00      1.00      1.00         4\n",
      "     navigation_instances_tab       0.00      0.00      0.00         5\n",
      "    navigation_main_dashboard       1.00      1.00      1.00         5\n",
      "      navigation_payments_tab       0.80      0.80      0.80         5\n",
      "         navigation_tasks_tab       1.00      1.00      1.00         4\n",
      "                 out_of_scope       1.00      0.25      0.40         4\n",
      "                      pairing       0.00      0.00      0.00         4\n",
      "               payment_extend       0.56      1.00      0.71         5\n",
      "              payment_history       1.00      0.75      0.86         4\n",
      "                payment_issue       1.00      0.20      0.33         5\n",
      "              payment_methods       0.80      1.00      0.89         4\n",
      "               payment_status       0.80      1.00      0.89         4\n",
      "            payment_subscribe       0.60      0.75      0.67         4\n",
      "              payment_upgrade       1.00      0.80      0.89         5\n",
      "          platform_cloud_open       1.00      0.50      0.67         4\n",
      "             platform_compare       1.00      1.00      1.00         4\n",
      "        platform_desktop_info       1.00      1.00      1.00         5\n",
      "     platform_desktop_install       1.00      1.00      1.00         5\n",
      "             platform_updates       1.00      1.00      1.00         5\n",
      "         pricing_currency_idr       1.00      1.00      1.00         4\n",
      "      pricing_instance_limits       1.00      0.60      0.75         5\n",
      "                pricing_query       1.00      0.80      0.89         5\n",
      "                quota_reached       1.00      0.80      0.89         5\n",
      "          security_cloud_data       1.00      1.00      1.00         2\n",
      "             security_payment       1.00      1.00      1.00         4\n",
      "             security_privacy       0.83      1.00      0.91         5\n",
      "              support_contact       0.50      0.25      0.33         4\n",
      "                tasks_monitor       1.00      1.00      1.00         4\n",
      "                   tasks_view       0.44      1.00      0.62         4\n",
      "            tasks_view_failed       1.00      1.00      1.00         4\n",
      "             templates_create       0.62      1.00      0.77         5\n",
      "                templates_use       1.00      0.50      0.67         4\n",
      "                 tips_browser       0.71      1.00      0.83         5\n",
      "                tips_internet       1.00      1.00      1.00         5\n",
      "      troubleshoot_connection       1.00      1.00      1.00         5\n",
      "              troubleshoot_qr       0.56      1.00      0.71         5\n",
      "                warmup_enable       1.00      1.00      1.00         4\n",
      "                  warmup_info       1.00      1.00      1.00         4\n",
      "\n",
      "                     accuracy                           0.81       371\n",
      "                    macro avg       0.83      0.80      0.78       371\n",
      "                 weighted avg       0.84      0.81      0.79       371\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\Documents\\GitHub_Repository\\Python\\chattea-ai\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\nicho\\Documents\\GitHub_Repository\\Python\\chattea-ai\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\nicho\\Documents\\GitHub_Repository\\Python\\chattea-ai\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# TRAINING\n",
    "# ----------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    pr, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": pr, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_MODEL_DIR),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    fp16=FP16 and DEVICE == \"cuda\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "trainer.save_model(str(OUTPUT_MODEL_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_MODEL_DIR))\n",
    "print(\"Model and tokenizer saved to:\", OUTPUT_MODEL_DIR)\n",
    "\n",
    "# Evaluate on test\n",
    "print(\"Evaluating on test set...\")\n",
    "preds_output = trainer.predict(tokenized[\"test\"])\n",
    "y_true = preds_output.label_ids\n",
    "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
    "print(\"\\nClassification report on TEST:\")\n",
    "print(classification_report(y_true, y_pred, target_names=unique_intents))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53254ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building sentence-transformers embeddings and FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:01<00:00, 46.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (2469, 384)\n",
      "FAISS dimension: 384\n",
      "FAISS not available. Install faiss-cpu or faiss-gpu to enable semantic fallback.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# BUILD FAISS INDEX (sentence-transformers) from all texts (train+val+test)\n",
    "# ----------------------------------------\n",
    "print(\"\\nBuilding sentence-transformers embeddings and FAISS index...\")\n",
    "embedder = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "all_texts = (train_df[\"text\"].tolist() + val_df[\"text\"].tolist() + test_df[\"text\"].tolist())\n",
    "all_intents = (train_df[\"intent\"].tolist() + val_df[\"intent\"].tolist() + test_df[\"intent\"].tolist())\n",
    "\n",
    "# Preprocess texts consistently\n",
    "all_texts_proc = [preprocess_text(t) for t in all_texts]\n",
    "\n",
    "embeddings = embedder.encode(all_texts_proc, convert_to_numpy=True, show_progress_bar=True)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "np.save(EMBEDDINGS_PATH, embeddings)\n",
    "# Save text-intent mapping\n",
    "index_to_item = [{\"text\": t, \"intent\": it} for t, it in zip(all_texts_proc, all_intents)]\n",
    "with open(TEXTS_INTENTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(index_to_item, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Build FAISS index (L2 normalized inner product is fine for cosine)\n",
    "d = embeddings.shape[1]\n",
    "print(\"FAISS dimension:\", d)\n",
    "# normalize embeddings for cosine\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "norms[norms == 0] = 1e-10\n",
    "embeddings_norm = embeddings / norms\n",
    "\n",
    "if FAISS_AVAILABLE:\n",
    "    # use IndexFlatIP on normalized vectors -> cosine similarity\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(embeddings_norm.astype('float32'))\n",
    "    faiss.write_index(index, str(FAISS_INDEX_PATH))\n",
    "    print(\"FAISS index saved to:\", FAISS_INDEX_PATH)\n",
    "else:\n",
    "    index = None\n",
    "    print(\"FAISS not available. Install faiss-cpu or faiss-gpu to enable semantic fallback.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc37daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config id2label sample: [(0, 'account_setup'), (1, 'advanced_contact_segmentation'), (2, 'analytics_view'), (3, 'api_reference'), (4, 'api_send_message')]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# INFERENCE: classifier + FAISS fallback\n",
    "# ----------------------------------------\n",
    "# reload label mappings into model config if needed\n",
    "# Ensure model.config.id2label is present (Trainer set it earlier)\n",
    "print(\"Model config id2label sample:\", list(model.config.id2label.items())[:5])\n",
    "\n",
    "def classifier_predict_intent(text):\n",
    "    \"\"\"\n",
    "    Returns (intent_key, confidence)\n",
    "    confidence = softmax probability of predicted class\n",
    "    \"\"\"\n",
    "    text_pre = preprocess_text(text)\n",
    "    inputs = tokenizer(text_pre, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        pred_idx = int(probs.argmax())\n",
    "        confidence = float(probs[pred_idx])\n",
    "        intent_key = model.config.id2label[pred_idx]\n",
    "    return intent_key, confidence\n",
    "\n",
    "def faiss_fallback(text, top_k=TOP_K_FAISS):\n",
    "    \"\"\"\n",
    "    Returns the intent of the top retrieved item from FAISS (cosine)\n",
    "    \"\"\"\n",
    "    if index is None:\n",
    "        return None, 0.0\n",
    "    q = preprocess_text(text)\n",
    "    q_emb = embedder.encode([q], convert_to_numpy=True)[0]\n",
    "    qn = q_emb / (np.linalg.norm(q_emb) + 1e-10)\n",
    "    qn = qn.astype('float32').reshape(1, -1)\n",
    "    D, I = index.search(qn, top_k)  # inner product scores, indices\n",
    "    # take top result\n",
    "    best_idx = int(I[0][0])\n",
    "    score = float(D[0][0])\n",
    "    mapped = index_to_item[best_idx]\n",
    "    return mapped[\"intent\"], float(score)\n",
    "\n",
    "def get_bilingual_response(intent_key, user_lang):\n",
    "    \"\"\"\n",
    "    Return response string for intent and language with fallbacks.\n",
    "    responses_bilingual: dict(intent -> {\"id\": \"...\", \"en\": \"...\"})\n",
    "    \"\"\"\n",
    "    entry = responses_bilingual.get(intent_key, {})\n",
    "    if not isinstance(entry, dict):\n",
    "        # in case responses file stores direct strings for some intents\n",
    "        return entry if entry else \"Maaf, saya belum memiliki jawaban.\"\n",
    "    # normalize keys\n",
    "    entry_norm = {k.lower(): v for k, v in entry.items()}\n",
    "    resp = entry_norm.get(user_lang)\n",
    "    if resp:\n",
    "        return resp\n",
    "    return entry_norm.get(\"id\") or entry_norm.get(\"en\") or \"Maaf, saya belum memiliki jawaban.\"\n",
    "\n",
    "def predict_and_respond(text, confidence_threshold=CONFIDENCE_THRESHOLD):\n",
    "    # step 1: classifier\n",
    "    intent_cls, conf = classifier_predict_intent(text)\n",
    "    chosen_intent = intent_cls\n",
    "    source = \"classifier\"\n",
    "    # if low confidence, try FAISS\n",
    "    if conf < confidence_threshold and FAISS_AVAILABLE:\n",
    "        intent_faiss, score = faiss_fallback(text, top_k=TOP_K_FAISS)\n",
    "        if intent_faiss is not None:\n",
    "            # we can compare raw scores (cosine) vs classifier confidence scaled â€” simple rule:\n",
    "            # if faiss returns something, use it (or apply a small acceptance rule)\n",
    "            chosen_intent = intent_faiss\n",
    "            source = \"faiss\"\n",
    "    # choose language and response\n",
    "    lang = detect_language_simple(text)\n",
    "    response_text = get_bilingual_response(chosen_intent, lang)\n",
    "    return {\"intent\": chosen_intent, \"confidence\": conf, \"source\": source, \"response\": response_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6595aded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEMO ---\n",
      "\n",
      "User: gimana cara blast message?\n",
      "-> intent: cancel_action (conf=0.014) [source=classifier]\n",
      "-> response: Okay, canceled. Anything else I can help with?\n",
      "\n",
      "User: how to schedule messages?\n",
      "-> intent: message_schedule (conf=0.026) [source=classifier]\n",
      "-> response: To schedule a message:\n",
      "1. Write the message as usual\n",
      "2. Click the 'Schedule' icon (â°)\n",
      "3. Choose the send date & time\n",
      "4. Click 'Schedule Message'\n",
      "\n",
      "The message will be sent automatically on schedule!\n",
      "\n",
      "Example: Schedule a promo blast for tomorrow at 09:00\n",
      "\n",
      "User: cek nomor wa yang valid\n",
      "-> intent: function_check_phone (conf=0.018) [source=classifier]\n",
      "-> response: Untuk cek nomor terdaftar di WhatsApp:\n",
      "1. Buka menu 'Tools' â†’ 'Phone Checker'\n",
      "2. Input nomor atau upload file CSV\n",
      "3. Klik 'Check'\n",
      "4. Lihat hasil: Valid (âœ…) atau Invalid (âŒ)\n",
      "\n",
      "Fitur ini hemat waktu sebelum blast message!\n",
      "\n",
      "User: tolong bantu saya\n",
      "-> intent: support_contact (conf=0.015) [source=classifier]\n",
      "-> response: Need help? Contact Chattea Support:\n",
      "\n",
      "ðŸ“§ Email: support@chattea.com\n",
      "ðŸ’¬ Live Chat: https://chattea.com (click the chat icon)\n",
      "ðŸ“± WhatsApp: +62 812-3456-7890\n",
      "ðŸ¦ Twitter: @ChatteatSupport\n",
      "\n",
      "â° Operating Hours:\n",
      "- Monâ€“Fri: 09:00â€“18:00 WIB\n",
      "- Sat: 09:00â€“15:00 WIB\n",
      "- Sun: Closed (emergency only)\n",
      "\n",
      "ðŸ’¼ Enterprise Support: 24/7\n",
      "\n",
      "Weâ€™re ready to help! ðŸ˜Š\n",
      "\n",
      "User: what is the pricing plan?\n",
      "-> intent: pricing_query (conf=0.022) [source=classifier]\n",
      "-> response: Chattea pricing:\n",
      "\n",
      "ðŸ’° Plans:\n",
      "\n",
      "ðŸ“± Basic â€” Rp 99K/month\n",
      "ðŸš€ Pro â€” Rp 299K/month\n",
      "ðŸ’¼ Enterprise â€” Rp 999K/month\n",
      "\n",
      "What you get:\n",
      "- Basic: 1 instance, 1K messages\n",
      "- Pro: 5 instances, unlimited messages\n",
      "- Enterprise: Unlimited everything + 24/7 support\n",
      "\n",
      "ðŸŽ Promo: FREE 7-day trial!\n",
      "ðŸ’¡ Save: Yearly plan 20% off\n",
      "\n",
      "Sign up: https://chattea.com/signup\n",
      "\n",
      "Script finished. Artifacts saved to: c:\\Users\\nicho\\Documents\\GitHub_Repository\\Python\\chattea-ai\\artifacts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# DEMO\n",
    "# ----------------------------------------\n",
    "print(\"\\n--- DEMO ---\")\n",
    "demo_queries = [\n",
    "    \"gimana cara blast message?\",\n",
    "    \"how to schedule messages?\",\n",
    "    \"cek nomor wa yang valid\",\n",
    "    \"tolong bantu saya\",\n",
    "    \"what is the pricing plan?\"\n",
    "]\n",
    "\n",
    "for q in demo_queries:\n",
    "    res = predict_and_respond(q)\n",
    "    print(f\"\\nUser: {q}\")\n",
    "    print(f\"-> intent: {res['intent']} (conf={res['confidence']:.3f}) [source={res['source']}]\")\n",
    "    print(f\"-> response: {res['response']}\")\n",
    "\n",
    "print(\"\\nScript finished. Artifacts saved to:\", ARTIFACTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c62abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Can you help me upgrade my plan.?\n",
      "True: payment_upgrade | Pred: payment_upgrade | Conf: 0.027\n",
      "\n",
      "Text: List all my instances? Please thank you\n",
      "True: instance_list | Pred: instance_list | Conf: 0.029\n",
      "\n",
      "Text: Access chattea in browser please\n",
      "True: platform_cloud_open | Pred: definition | Conf: 0.024\n",
      "\n",
      "Text: Could you scan qr code to connect whatsapp thanks\n",
      "True: pairing | Pred: troubleshoot_qr | Conf: 0.029\n",
      "\n",
      "Text: What's the best way to add contact?\n",
      "True: contacts_add | Pred: contacts_add | Conf: 0.033\n",
      "\n",
      "Text: How can I internet requirements\n",
      "True: tips_internet | Pred: tips_internet | Conf: 0.026\n",
      "\n",
      "Text: Help me send a message\n",
      "True: function_send_message | Pred: chat_send | Conf: 0.027\n",
      "\n",
      "Text: I'd like to payment status check\n",
      "True: payment_status | Pred: payment_status | Conf: 0.030\n",
      "\n",
      "Text: Make a whatsapp group named 'vip customers' thanks\n",
      "True: group_create | Pred: group_create | Conf: 0.021\n",
      "\n",
      "Text: Could you show me how to improve performance thank\n",
      "True: general_tips | Pred: general_tips | Conf: 0.025\n",
      "\n",
      "Text: I want to go to dashboard thank you\n",
      "True: navigation_main_dashboard | Pred: navigation_main_dashboard | Conf: 0.030\n",
      "\n",
      "Text: How to sign up for Chattea.\n",
      "True: account_setup | Pred: definition | Conf: 0.020\n",
      "\n",
      "Text: Can you help me transaction log.\n",
      "True: payment_history | Pred: payment_history | Conf: 0.020\n",
      "\n",
      "Text: Describe chattea briefly. Please thank you\n",
      "True: definition | Pred: definition | Conf: 0.033\n",
      "\n",
      "Text: Talk to support team\n",
      "True: support_contact | Pred: support_contact | Conf: 0.021\n",
      "\n",
      "Text: I need help with find contacts please\n",
      "True: contacts_filter | Pred: contacts_manage | Conf: 0.027\n",
      "\n",
      "Text: What's the best way to which browser is recommende\n",
      "True: tips_browser | Pred: tips_browser | Conf: 0.035\n",
      "\n",
      "Text: Could you view payment info\n",
      "True: navigation_payments_tab | Pred: payment_methods | Conf: 0.025\n",
      "\n",
      "Text: Apply template\n",
      "True: templates_use | Pred: templates_create | Conf: 0.027\n",
      "\n",
      "Text: Display profile details thanks\n",
      "True: instance_view_profile | Pred: instance_view_profile | Conf: 0.028\n",
      "\n",
      "Accuracy: 65.0%\n",
      "High confidence: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Test on validation set instead of random queries\n",
    "correct = 0\n",
    "high_conf = 0\n",
    "\n",
    "for _, row in val_df.head(20).iterrows():\n",
    "    intent_pred, conf = classifier_predict_intent(row['text'])\n",
    "    print(f\"Text: {row['text'][:50]}\")\n",
    "    print(f\"True: {row['intent']} | Pred: {intent_pred} | Conf: {conf:.3f}\\n\")\n",
    "    \n",
    "    if intent_pred == row['intent']:\n",
    "        correct += 1\n",
    "    if conf > 0.5:\n",
    "        high_conf += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/20:.1%}\")\n",
    "print(f\"High confidence: {high_conf/20:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d514e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
